{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339b9f1f",
   "metadata": {},
   "source": [
    "# üöÄ Requ√™te WIKIDATA\n",
    "\n",
    "## üìë Mode d'emploi\n",
    "\n",
    "Suivre les instructions au fil du notebook et ex√©cuter une √† une les cellules de code en appuyant sur la petite fl√®che √† gauche (‚ñ∂Ô∏è)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f576147d",
   "metadata": {},
   "source": [
    "## üî® Construction de l'environnement n√©cessaire et configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74111c85",
   "metadata": {},
   "source": [
    "### Installation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdaf8794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Installation termin√©e !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# üì¶ MODULES NECESSAIRES : NORMALEMENT, NE RUN QU'A LA PREMIERE UTILISATION\n",
    "%pip install -q SPARQLWrapper tqdm pandas\n",
    "\n",
    "print(\"‚úÖ Installation termin√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e7bad",
   "metadata": {},
   "source": [
    "### Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7becfc5d",
   "metadata": {},
   "source": [
    "#### Param√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e86a25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Imports termin√©s !\n"
     ]
    }
   ],
   "source": [
    "# üîß IMPORTS PYTHON\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üîß Imports termin√©s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f700d27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "                                      üöÄ CONFIGURATION TERMIN√âE                                      \n",
      "====================================================================================================\n",
      "üìÅ Dossier de sortie                  ./output\n",
      "‚è±Ô∏è  Rate limit                        3.0s entre requ√™tes\n",
      "üì¶ Taille des batches                 10\n",
      "üîÑ Nombre maximal de tentatives       3\n",
      "‚è≥ D√©lai de timeout des requ√™tes      60s\n",
      "üîç Taille du batch d'enrich.         15\n",
      "üîÅ Limite de boucle                  25 it√©rations\n",
      "üåê Endpoint Wikidata                 https://query.wikidata.org/sparql\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# üîß CONFIGURATION PERSONNALISABLE DE LA REQUETE\n",
    "\n",
    "WIKIDATA_ENDPOINT = \"https://query.wikidata.org/sparql\" # Endpoint SPARQL de Wikidata\n",
    "RATE_LIMIT_DELAY = 3.0  # D√©lai entre les requ√™tes\n",
    "BATCH_SIZE = 10  # Taille des batches pour les requ√™tes\n",
    "MAX_RETRIES = 3  # Nombre maximal de tentatives en cas d'√©chec\n",
    "REQUEST_TIMEOUT = 60 # Temps au bout duquel un requ√™te s'arr√™te automatiquement s'il n'y a pas de r√©ponse (en secondes)\n",
    "ENRICHMENT_BATCH_SIZE = 15 # Taille du batch pour l'enrichissement des donn√©es\n",
    "LOOP_LIMIT = 25 # Nombre r√©ponses limite par boucle (permet de requ√™ter petit √† petit pour ne pas surcharger l'API)\n",
    "LOOP_OFFSET = 0 # D√©calage pour la pagination des r√©sultats\n",
    "MAX_RESULTS = 50  # Nombre maximal de r√©sultats √† r√©cup√©rer (pour √©viter de surcharger l'API)\n",
    "\n",
    "\n",
    "# üìÅ Configuration du dossier de sortie \n",
    "output_dir = \"./output\" # Param√®tre √† remplacer si vous souhaitez un autre dossier de sortie (faire un copier coller du chemin d'un fichier)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"üöÄ CONFIGURATION TERMIN√âE\".center(100))\n",
    "print(\"=\"*100)\n",
    "print(\n",
    "    f\"üìÅ Dossier de sortie                  {output_dir}\\n\"\n",
    "    f\"‚è±Ô∏è  Rate limit                        {RATE_LIMIT_DELAY}s entre requ√™tes\\n\"\n",
    "    f\"üì¶ Taille des batches                 {BATCH_SIZE}\\n\"\n",
    "    f\"üîÑ Nombre maximal de tentatives       {MAX_RETRIES}\\n\"\n",
    "    f\"‚è≥ D√©lai de timeout des requ√™tes      {REQUEST_TIMEOUT}s\\n\"\n",
    "    f\"üîç Taille du batch d'enrich.         {ENRICHMENT_BATCH_SIZE}\\n\"\n",
    "    f\"üîÅ Limite de boucle                  {LOOP_LIMIT} it√©rations\\n\"\n",
    "    f\"üåê Endpoint Wikidata                 {WIKIDATA_ENDPOINT}\"\n",
    ")\n",
    "print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27ffccf",
   "metadata": {},
   "source": [
    "#### Client sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "519640a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonctions de requ√™te SPARQL pr√™tes !\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è FONCTIONS DE PARAMETRAGE DES REQU√äTES ET DU CARNET\n",
    "\n",
    "################################################################################\n",
    "# FONCTION POUR CR√âER UN CLIENT SPARQL\n",
    "################################################################################\n",
    "def create_sparql_client():\n",
    "    \"\"\"\n",
    "    Cr√©e un client SPARQL pour interagir avec Wikidata\n",
    "    :return: Instance de SPARQLWrapper configur√©e pour Wikidata\n",
    "    \"\"\"\n",
    "    sparql = SPARQLWrapper(WIKIDATA_ENDPOINT)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.setTimeout(REQUEST_TIMEOUT)\n",
    "    return sparql\n",
    "\n",
    "################################################################################\n",
    "# FONCTION PRINCIPALE POUR EX√âCUTER UNE REQU√äTE SPARQL\n",
    "################################################################################\n",
    "def execute_sparql_query(query, max_retries=MAX_RETRIES, use_pagination=False, limit=None, max_results=None):\n",
    "    \"\"\"\n",
    "    Ex√©cute une requ√™te SPARQL avec gestion des erreurs, rate limiting et pagination optionnelle\n",
    "    :param query: La requ√™te SPARQL √† ex√©cuter\n",
    "    :param max_retries: Nombre maximum de tentatives en cas d'√©chec\n",
    "    :param use_pagination: Si True, active la pagination automatique\n",
    "    :param limit: Taille des pages pour la pagination (d√©faut: LOOP_LIMIT)\n",
    "    :param max_results: Nombre maximum de r√©sultats √† r√©cup√©rer (None = illimit√©)\n",
    "    :return: R√©sultats de la requ√™te ou une liste vide en cas d'√©chec\n",
    "    \"\"\"\n",
    "    sparql = create_sparql_client()\n",
    "    \n",
    "    # MODE SIMPLE SANS PAGINATION\n",
    "    if not use_pagination:\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # D√©finir la requ√™te SPARQL √† ex√©cuter\n",
    "                sparql.setQuery(query)\n",
    "                # Ex√©cuter la requ√™te et convertir les r√©sultats au format JSON\n",
    "                results = sparql.query().convert()\n",
    "                # Attendre pour respecter le rate limiting\n",
    "                time.sleep(RATE_LIMIT_DELAY)\n",
    "                # Retourner les r√©sultats extraits\n",
    "                return results[\"results\"][\"bindings\"]\n",
    "            except Exception as e:\n",
    "                # Afficher un message d'erreur en cas d'√©chec\n",
    "                print(f\"‚ö†Ô∏è  Tentative {attempt + 1}/{max_retries} √©chou√©e: {e}...\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    # Attendre plus longtemps avant la prochaine tentative\n",
    "                    time.sleep(RATE_LIMIT_DELAY * (attempt + 2))\n",
    "                else:\n",
    "                    # Afficher un message d'√©chec apr√®s toutes les tentatives\n",
    "                    print(f\"‚ùå Requ√™te √©chou√©e apr√®s {max_retries} tentatives\")\n",
    "                    return []\n",
    "        return []\n",
    "    \n",
    "    # MODE PAGINATION ACTIVE\n",
    "    if limit is None:\n",
    "        limit = LOOP_LIMIT\n",
    "    \n",
    "    all_results = []\n",
    "    offset = 0\n",
    "    \n",
    "    print(f\"üîç D√©but de la pagination (limit={limit})...\")\n",
    "    \n",
    "    while True:\n",
    "        # Ajouter LIMIT et OFFSET √† la requ√™te\n",
    "        paginated_query = f\"{query.rstrip()} LIMIT {limit} OFFSET {offset}\"\n",
    "        \n",
    "        print(f\"üîπ Requ√™te OFFSET {offset}, LIMIT {limit}\")\n",
    "        \n",
    "        success = False\n",
    "        # Essayer d'ex√©cuter la requ√™te avec pagination\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # D√©finir la requ√™te SPARQL √† ex√©cuter\n",
    "                sparql.setQuery(paginated_query)\n",
    "                # Ex√©cuter la requ√™te et convertir les r√©sultats au format JSON\n",
    "                results = sparql.query().convert()\n",
    "                # Extraire les r√©sultats\n",
    "                bindings = results[\"results\"][\"bindings\"]\n",
    "                \n",
    "                if not bindings:\n",
    "                    # Si aucun r√©sultat, on consid√®re que la pagination est termin√©e\n",
    "                    print(\"‚úÖ Fin de la pagination - Aucun r√©sultat suppl√©mentaire.\")\n",
    "                    success = True\n",
    "                    break\n",
    "                \n",
    "                # Afficher le nombre de r√©sultats r√©cup√©r√©s\n",
    "                all_results.extend(bindings)\n",
    "                print(f\"‚úÖ R√©cup√©r√© {len(bindings)} r√©sultats (total: {len(all_results)})\")\n",
    "                \n",
    "                # V√©rifier la limite max_results\n",
    "                if max_results and len(all_results) >= max_results:\n",
    "                    print(f\"üéØ Limite de {max_results} r√©sultats atteinte.\")\n",
    "                    all_results = all_results[:max_results]  # Tronquer si n√©cessaire\n",
    "                    success = True\n",
    "                    break\n",
    "                \n",
    "                success = True\n",
    "                break\n",
    "\n",
    "            # # Gestion des erreurs    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Tentative {attempt + 1}/{max_retries} √©chou√©e √† l'offset {offset}: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(RATE_LIMIT_DELAY * (attempt + 2))\n",
    "                else:\n",
    "                    print(f\"‚ùå Requ√™te √©chou√©e apr√®s {max_retries} tentatives √† l'offset {offset}\")\n",
    "                    return all_results  # Retourner ce qu'on a r√©ussi √† r√©cup√©rer\n",
    "        \n",
    "        if not success or (max_results and len(all_results) >= max_results):\n",
    "            break\n",
    "        \n",
    "        # Incr√©menter l'offset pour la prochaine page\n",
    "        offset += limit\n",
    "        \n",
    "        # Rate limiting pour √©viter de surcharger la requ√™te\n",
    "        print(f\"‚è≥ Attente de {RATE_LIMIT_DELAY}s...\")\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    print(f\"üéØ Total final r√©cup√©r√© : {len(all_results)} r√©sultats.\")\n",
    "    return all_results\n",
    "\n",
    "################################################################################\n",
    "# FONCTION UTILITAIRE POUR EXTRAIRE LES ID DE WIKIDATA\n",
    "################################################################################\n",
    "def clean_entity_id(entity_uri):\n",
    "    \"\"\"\n",
    "    Extrait l'ID d'une entit√© √† partir de son URI\n",
    "    :param entity_uri: URI de l'entit√© (ex: \"http://www.wikidata.org/entity/Q42'\")\n",
    "    :return: ID de l'entit√© (ex: \"Q42\") ou une cha√Æne vide si l'URI est vide\n",
    "    \"\"\"\n",
    "    if not entity_uri:\n",
    "        return \"\"\n",
    "    return entity_uri.split(\"/\")[-1] if \"/\" in entity_uri else entity_uri\n",
    "\n",
    "################################################################################\n",
    "# FONCTION POUR EX√âCUTER DES REQU√äTES SPARQL EN BATCH : VERIFIER L'UTILITE\n",
    "################################################################################\n",
    "def execute_batch_queries(queries, description=\"Requ√™tes\", use_pagination=False):\n",
    "    \"\"\"\n",
    "    Ex√©cute une liste de requ√™tes SPARQL en batch\n",
    "    :param queries: Requ√™te SPARQL unique ou liste de requ√™tes\n",
    "    :param description: Description de la t√¢che pour le logging\n",
    "    :param use_pagination: Si True, active la pagination pour chaque requ√™te\n",
    "    :return: Liste de tous les r√©sultats combin√©s\n",
    "    \"\"\"\n",
    "    # V√©rifier si queries est une string ou une liste\n",
    "    if isinstance(queries, str):\n",
    "        # Si c'est une string, c'est une seule requ√™te\n",
    "        print(f\"üîπ Ex√©cution d'une requ√™te unique: {description}\")\n",
    "        return execute_sparql_query(queries, use_pagination=use_pagination)\n",
    "    \n",
    "    # Si c'est une liste, traiter comme batch\n",
    "    all_results = []\n",
    "    for i, query in enumerate(tqdm(queries, desc=description)):\n",
    "        results = execute_sparql_query(query, use_pagination=use_pagination)\n",
    "        all_results.extend(results)\n",
    "        if (i + 1) % BATCH_SIZE == 0:\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "    return all_results\n",
    "\n",
    "################################################################################\n",
    "# FONCTION POUR EX√âCUTER UNE REQU√äTE SPARQL AVEC PAGINATION\n",
    "################################################################################\n",
    "def execute_paginated_query(base_query, limit=None, max_results=MAX_RESULTS):\n",
    "    \"\"\"\n",
    "    Fonction helper pour ex√©cuter facilement une requ√™te avec pagination\n",
    "    :param base_query: Requ√™te SPARQL de base (sans LIMIT/OFFSET)\n",
    "    :param limit: Taille des pages (d√©faut: LOOP_LIMIT)\n",
    "    :param max_results: Nombre maximum de r√©sultats (None = illimit√©)\n",
    "    :return: Liste de tous les r√©sultats\n",
    "    \"\"\"\n",
    "    return execute_sparql_query(\n",
    "        base_query, \n",
    "        use_pagination=True, \n",
    "        limit=limit, \n",
    "        max_results=max_results\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Fonctions de requ√™te SPARQL pr√™tes !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d392e0",
   "metadata": {},
   "source": [
    "## üßê REQU√äTES\n",
    "\n",
    "R√©sum√© des requ√™tes disponibles : \n",
    "| Type de requ√™te | R√©sum√© | Output |\n",
    "| --- | --- | --- |\n",
    "| Recherche par nom |  `SELECT ?item ?itemLabel WHERE { ?item rdfs:label ?itemLabel. FILTER(LANG(?itemLabel) = \"en\"). FILTER(CONTAINS(LCASE(?itemLabel), \"{search_entity_name}\")). }` | Label, ID\n",
    "\n",
    "\n",
    "### 1. Aide √† la requ√™te : Rechercher une entit√© par nom \n",
    "\n",
    "Cette fonction peut s'utiliser pour retirer tous les termes wikidata qui comprennent une cha√Æne de caract√®re\n",
    "1. Dans leur label\n",
    "2. Dans un de leurs termes g√©n√©riques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e118180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üîç RECHERCHE DES TERMES CONTENANT LA CHAINE DE CARACTERE :  'aero'...\n",
      "====================================================================================================\n",
      "üîç REQU√äTE ENVOYEE :\n",
      "        SELECT ?item ?itemLabel\n",
      "        WHERE {\n",
      "            ?item wdt:P31*/wdt:P279* ?parent.\n",
      "            ?parent rdfs:label ?parentLabel.\n",
      "            FILTER(LANG(?parentLabel) = \"en\").\n",
      "            FILTER(CONTAINS(LCASE(?parentLabel), \"aero\")).\n",
      "            SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
      "        }\n",
      "        \n",
      "====================================================================================================\n",
      "üîç D√©but de la pagination (limit=25)...\n",
      "üîπ Requ√™te OFFSET 0, LIMIT 25\n",
      "‚ö†Ô∏è  Tentative 1/3 √©chou√©e √† l'offset 0: The read operation timed out\n",
      "‚ö†Ô∏è  Tentative 2/3 √©chou√©e √† l'offset 0: The read operation timed out\n",
      "‚ö†Ô∏è  Tentative 3/3 √©chou√©e √† l'offset 0: HTTP Error 429: Too Many Requests\n",
      "‚ùå Requ√™te √©chou√©e apr√®s 3 tentatives √† l'offset 0\n",
      "====================================================================================================\n",
      "‚úÖ RESULTATS SAUVEGARDES DANS ./output\\20250703_104200_RAW_aero.json\n"
     ]
    }
   ],
   "source": [
    "# üîé RECHERCHE PAR NOM\n",
    "# =================================================================================\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# Demande √† l'utilisateur le nom de l'entit√© √† rechercher dans Wikidata\n",
    "# =================================================================================\n",
    "search_entity_name = input(\"Entrez le nom de l'entit√© √† rechercher dans wikidata, en anglais (ex: aeronautics) : \").strip()\n",
    "if not search_entity_name:\n",
    "    print(\"‚ùå Aucun nom d'entit√© fourni.\")\n",
    "    exit(1)  # Sortir du script si aucun nom n'est fourni\n",
    "\n",
    "# =================================================================================\n",
    "# DEFINITION DE REQU√äTE SPARQL POUR RECHERCHER UNE ENTIT√â PAR NOM\n",
    "# =================================================================================\n",
    "\n",
    "query_by_label = f\"\"\"\n",
    "        SELECT ?item ?itemLabel\n",
    "        WHERE {{\n",
    "            ?item rdfs:label ?itemLabel.\n",
    "            FILTER(LANG(?itemLabel) = \"en\").\n",
    "            FILTER(CONTAINS(LCASE(?itemLabel), \"{search_entity_name}\")).\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "query_by_parent = f\"\"\"\n",
    "        SELECT ?item ?itemLabel\n",
    "        WHERE {{\n",
    "            ?item wdt:P31*/wdt:P279* ?parent.\n",
    "            ?parent rdfs:label ?parentLabel.\n",
    "            FILTER(LANG(?parentLabel) = \"en\").\n",
    "            FILTER(CONTAINS(LCASE(?parentLabel), \"{search_entity_name}\")).\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "# =================================================================================\n",
    "# DEMANDE √Ä L'UTILISATEUR LE TYPE DE REQU√äTE √Ä UTILISER\n",
    "# =================================================================================\n",
    "\n",
    "# Choix de la requ√™te √† utiliser\n",
    "query_choice = input(\n",
    "    \"Quel type de requ√™te utiliser ?\\n\"\n",
    "    \"1Ô∏è‚É£ Recherche d'une cha√Æne dans les labels\\n\"\n",
    "    \"2Ô∏è‚É£ Requ√™te d'une cha√Æne dans les parents\\n\"\n",
    "    \"Entrez le num√©ro de votre choix (1 ou 2) : \"\n",
    ").strip()\n",
    "if query_choice not in [\"1\", \"2\"]:\n",
    "    print(\"‚ùå Choix invalide. Veuillez entrer 1 ou 2.\")\n",
    "    exit(1)  # Sortir du script si le choix est invalide\n",
    "\n",
    "if query_choice == \"1\":\n",
    "    query_regex = query_by_label\n",
    "elif query_choice == \"2\":\n",
    "    query_regex = query_by_parent\n",
    "\n",
    "\n",
    "def find_entity_by_name(search_entity_name=search_entity_name, query_regex=query_regex):\n",
    "    \"\"\"\n",
    "    Demande √† l'utilisateur le nom de l'entit√© √† rechercher dans Wikidata\n",
    "    :return: Liste des r√©sultats de la recherche\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(f\"üîç RECHERCHE DES TERMES CONTENANT LA CHAINE DE CARACTERE :  '{search_entity_name}'...\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"üîç REQU√äTE ENVOYEE :{query_regex}\")\n",
    "    print(\"=\" * 100)\n",
    "    return execute_paginated_query(query_regex)\n",
    "raw_entity_by_name_results = find_entity_by_name()\n",
    "\n",
    "# =================================================================================\n",
    "# üì© SAUVEGARDE DES R√âSULTATS EN JSON\n",
    "# =================================================================================\n",
    "\n",
    "def save_raw_results_to_json(raw_entity_by_name_results, search_entity_name):\n",
    "    \"\"\"\n",
    "    Sauvegarde les r√©sultats bruts de la recherche dans un fichier JSON\n",
    "    :param raw_entity_by_name_results: R√©sultats bruts de la recherche\n",
    "    :param search_entity_name: Nom de l'entit√© recherch√©e\n",
    "    \"\"\"\n",
    "    # Cr√©ation du nom de fichier avec la date et l'heure actuelles\n",
    "    raw_json_filename = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_RAW_{search_entity_name}.json\"\n",
    "    raw_json_filepath = os.path.join(output_dir, raw_json_filename)\n",
    "\n",
    "    with open(raw_json_filepath, 'w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(raw_entity_by_name_results, jsonfile, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(f\"‚úÖ RESULTATS SAUVEGARDES DANS {raw_json_filepath}\")\n",
    "save_raw_results_to_json(raw_entity_by_name_results, search_entity_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad135870",
   "metadata": {},
   "source": [
    "### 2. Aide √† la requ√™te : Trouver toutes les propri√©t√©s dans lesquelles un terme est utilis√©\n",
    "\n",
    "```sql\n",
    "SELECT ?item ?itemLabel ?prop ?propLabel WHERE {\n",
    "  ?item ?prop wd:Q936518.\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"fr\". }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "```sql\n",
    "SELECT ?prop ?propLabel ?exemple ?exempleLabel WHERE {\n",
    "  {\n",
    "    SELECT ?prop (SAMPLE(?item) AS ?exemple) WHERE {\n",
    "      ?item ?prop wd:Q936518.\n",
    "    }\n",
    "    GROUP BY ?prop\n",
    "  }\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"fr\". }\n",
    "}\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81aa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUETE PAR PROPRIETE\n",
    "# =================================================================================\n",
    "\n",
    "search_term_by_id = input(\"Entrez l'ID de l'entit√© √† rechercher dans wikidata (ex: Q42) : \").strip()\n",
    "# Validation de l'ID de l'entit√©\n",
    "if not search_term_by_id or not re.match(r\"^Q\\d+$\", search_term_by_id):\n",
    "    print(\"‚ùå ID d'entit√© invalide. Veuillez entrer un ID valide (ex: Q42).\")\n",
    "    exit(1)  # Sortir du script si l'ID est invalide\n",
    "\n",
    "query_by_property = f\"\"\" \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2c66f",
   "metadata": {},
   "source": [
    "### REQU√äTES THEMATIQUES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295aa2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_aeronautics_extraction_queries():\n",
    "    \"\"\"Construit les requ√™tes d'extraction des donn√©es\"\"\"\n",
    "    queries = {\n",
    "        \"manufacturers\": \"\"\"\n",
    "        SELECT DISTINCT ?item \n",
    "              \n",
    "                ?itemLabel\n",
    "               (COALESCE(?itemDescription_fr, ?itemDescription_en, ?itemDescription_any, \"\") AS ?itemDescription)\n",
    "               ?parent\n",
    "               ?parentLabel\n",
    "               (GROUP_CONCAT(DISTINCT ?synonym_fr; separator=\",\") AS ?synonyms_fr)\n",
    "        WHERE {\n",
    "          # Tous les constructeurs d'aviation (instances ou sous-classes ou parties)\n",
    "          ?item wdt:P31*/wdt:P279*/wdt:P361*/wdt:P452*/wdt:P749* wd:Q936518 .\n",
    "        \n",
    "          # On cherche le parent imm√©diat selon diff√©rentes relations\n",
    "            OPTIONAL { ?item wdt:P361 ?partOf . }\n",
    "            OPTIONAL { ?item wdt:P279 ?subclassOf . }\n",
    "            OPTIONAL { ?item wdt:P31 ?instanceOf . }\n",
    "            OPTIONAL { ?item wdt:P452 ?secteur .}\n",
    "            OPTIONAL { ?item wdt:P176 ?constructeur.}\n",
    "            OPTIONAL { ?item wdt:P749 ?constructeur.}\n",
    "            \n",
    "            BIND(COALESCE(?partOf, ?subclassOf, ?instanceOf, ?secteur, ?constructeur) AS ?parent)\n",
    "        \n",
    "          # Description de l'item (fr > en > autre)\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_fr . FILTER(LANG(?itemDescription_fr) = \"fr\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_en . FILTER(LANG(?itemDescription_en) = \"en\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_any .\n",
    "                     FILTER(LANG(?itemDescription_any) != \"fr\" && LANG(?itemDescription_any) != \"en\") }\n",
    "\n",
    "          # Synonymes en fran√ßais (P1709 est \"synonymes exacts\")\n",
    "          OPTIONAL { ?item skos:altLabel ?synonym_fr . FILTER(LANG(?synonym_fr) = \"fr\") }\n",
    "        \n",
    "          SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language \"fr,en,[AUTO_LANGUAGE]\"\n",
    "          }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel ?itemDescription_fr ?itemDescription_en ?itemDescription_any ?parent ?parentLabel\n",
    "        \"\"\",\n",
    "\n",
    "        \"aircraft_models\": \"\"\"\n",
    "        SELECT DISTINCT ?item \n",
    "               ?itemLabel\n",
    "               (COALESCE(?itemDescription_fr, ?itemDescription_en, ?itemDescription_any, \"\") AS ?itemDescription)\n",
    "               ?parent\n",
    "               ?parentLabel\n",
    "               (GROUP_CONCAT(DISTINCT ?synonym_fr; separator=\" , \") AS ?synonyms_fr)\n",
    "        WHERE {\n",
    "          # Tous les mod√®les d'avions (instances ou sous-classes ou parties)\n",
    "          ?item wdt:P31/wdt:P279* wd:Q11436 .\n",
    "          \n",
    "        \n",
    "          # On cherche le parent imm√©diat selon diff√©rentes relations         \n",
    "            OPTIONAL { ?item wdt:P179 ?series .}\n",
    "            OPTIONAL { ?item wdt:176 ?constructeur.}\n",
    "            OPTIONAL { ?item wdt:P31 ?instanceOf . } \n",
    "            OPTIONAL { ?item wdt:P361 ?partOf . }\n",
    "            OPTIONAL { ?item wdt:P279 ?subclassOf . }\n",
    "                       \n",
    "            BIND(COALESCE(?partOf, ?subclassOf, ?instanceOf, ?series, ?constructeur) AS ?parent)\n",
    "        \n",
    "          # Description de l'item (fr > en > autre)\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_fr . FILTER(LANG(?itemDescription_fr) = \"fr\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_en . FILTER(LANG(?itemDescription_en) = \"en\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_any .\n",
    "                     FILTER(LANG(?itemDescription_any) != \"fr\" && LANG(?itemDescription_any) != \"en\") }\n",
    "\n",
    "          # Synonymes en fran√ßais\n",
    "          OPTIONAL { ?item skos:altLabel ?synonym_fr . FILTER(LANG(?synonym_fr) = \"fr\") }\n",
    "        \n",
    "          SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language \"fr,en,[AUTO_LANGUAGE]\"\n",
    "          }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel ?itemDescription_fr ?itemDescription_en ?itemDescription_any ?parent ?parentLabel\n",
    "        \"\"\",\n",
    "\n",
    "        \"aircraft_components\": \"\"\"\n",
    "        SELECT DISTINCT ?item \n",
    "               ?itemLabel\n",
    "               (COALESCE(?itemDescription_fr, ?itemDescription_en, ?itemDescription_any, \"\") AS ?itemDescription)\n",
    "               ?parent\n",
    "               ?parentLabel\n",
    "               (GROUP_CONCAT(DISTINCT ?synonym_fr; separator=\" , \") AS ?synonyms_fr)\n",
    "        WHERE {\n",
    "          # Tous les √©quipements d'aviation (instances ou sous-classes ou parties)\n",
    "          ?item wdt:P31*/wdt:P279*/wdt:P361* wd:Q16693356 .\n",
    "        \n",
    "          # On cherche le parent imm√©diat selon diff√©rentes relations\n",
    "            OPTIONAL { ?item wdt:P361 ?partOf . }\n",
    "            OPTIONAL { ?item wdt:P279 ?subclassOf . }\n",
    "            OPTIONAL { ?item wdt:P31 ?instanceOf . }\n",
    "            OPTIONAL { ?item wdt:P452 ?secteur .}\n",
    "            OPTIONAL { ?item wdt:P176 ?constructeur.}\n",
    "            \n",
    "            BIND(COALESCE(?partOf, ?subclassOf, ?instanceOf, ?secteur, ?constructeur) AS ?parent)\n",
    "        \n",
    "          # Description de l'item (fr > en > autre)\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_fr . FILTER(LANG(?itemDescription_fr) = \"fr\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_en . FILTER(LANG(?itemDescription_en) = \"en\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_any .\n",
    "                     FILTER(LANG(?itemDescription_any) != \"fr\" && LANG(?itemDescription_any) != \"en\") }\n",
    "\n",
    "          # Synonymes en fran√ßais\n",
    "          OPTIONAL { ?item skos:altLabel ?synonym_fr . FILTER(LANG(?synonym_fr) = \"fr\") }\n",
    "        \n",
    "          SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language \"fr,en,[AUTO_LANGUAGE]\"\n",
    "          }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel ?itemDescription_fr ?itemDescription_en ?itemDescription_any ?parent ?parentLabel\n",
    "        \"\"\",\n",
    "\n",
    "        \"aeronautic_profession\": \"\"\"\n",
    "        SELECT DISTINCT ?item \n",
    "               ?itemLabel\n",
    "               (COALESCE(?itemDescription_fr, ?itemDescription_en, ?itemDescription_any, \"\") AS ?itemDescription)\n",
    "               ?parent\n",
    "               ?parentLabel\n",
    "               (GROUP_CONCAT(DISTINCT ?synonym_fr; separator=\" , \") AS ?synonyms_fr)\n",
    "        WHERE {\n",
    "        ?item wdt:P425* ?domaine.\n",
    "        VALUES ?domaine { wd:Q765633 wd:Q906438 wd:Q1434048 wd:Q206814 wd:Q627716 wd:Q221395 wd:Q765633 wd:Q22719}.  \n",
    "        \n",
    "          # On cherche le parent imm√©diat selon diff√©rentes relations\n",
    "            OPTIONAL { ?item wdt:P361 ?partOf . }\n",
    "            OPTIONAL { ?item wdt:P279 ?subclassOf . }\n",
    "            OPTIONAL { ?item wdt:P31 ?instanceOf . }\n",
    "            OPTIONAL { ?item wdt:P452 ?secteur .}\n",
    "            OPTIONAL { ?item wdt:P176 ?constructeur.}\n",
    "            OPTIONAL { ?item wdt:P749 ?constructeur.}\n",
    "            \n",
    "            BIND(COALESCE(?partOf, ?subclassOf, ?instanceOf, ?secteur, ?constructeur, ?domaine) AS ?parent) # Attention √† coalesce pour √©viter les doublons\n",
    "        \n",
    "          # Description de l'item (fr > en > autre)\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_fr . FILTER(LANG(?itemDescription_fr) = \"fr\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_en . FILTER(LANG(?itemDescription_en) = \"en\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_any .\n",
    "                     FILTER(LANG(?itemDescription_any) != \"fr\" && LANG(?itemDescription_any) != \"en\") }\n",
    "\n",
    "          # Synonymes en fran√ßais\n",
    "          OPTIONAL { ?item skos:altLabel ?synonym_fr . FILTER(LANG(?synonym_fr) = \"fr\") }\n",
    "        \n",
    "          SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language \"fr,en,[AUTO_LANGUAGE]\"\n",
    "          }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel ?itemDescription_fr ?itemDescription_en ?itemDescription_any ?parent ?parentLabel\n",
    "        \"\"\"\n",
    "    }\n",
    "    return queries\n",
    "\n",
    "print(\"‚úÖ Requ√™tes d√©finies (avec synonymes fran√ßais)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea367aee",
   "metadata": {},
   "source": [
    "## üîé Lancer la recherche globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è EXTRACTION DES DONN√âES A√âRONAUTIQUES\n",
    "def extract_all_aeronautics_data():\n",
    "    \"\"\"Extrait toutes les donn√©es a√©ronautiques de mani√®re optimis√©e\"\"\"\n",
    "    print(\"üèóÔ∏è EXTRACTION HI√âRARCHIQUE EXHAUSTIVE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    queries = build_aeronautics_extraction_queries()\n",
    "    all_results = []\n",
    "    \n",
    "    for category, query in queries.items():\n",
    "        print(f\"\\nüîç Extraction: {category}\")\n",
    "        \n",
    "        # ‚úÖ CORRECTION: Utiliser execute_paginated_query au lieu de execute_batch_queries\n",
    "        results = execute_batch_queries(query)\n",
    "        \n",
    "        # Enrichir chaque r√©sultat avec sa cat√©gorie\n",
    "        for result in results:\n",
    "            result[\"source_category\"] = category\n",
    "        \n",
    "        all_results.extend(results)\n",
    "        print(f\"‚úÖ {len(results)} entit√©s trouv√©es pour {category}\")\n",
    "    \n",
    "    print(f\"\\nüéØ TOTAL: {len(all_results)} entit√©s extraites\")\n",
    "    return all_results\n",
    "\n",
    "raw_aeronautics_data = extract_all_aeronautics_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dffabe",
   "metadata": {},
   "source": [
    "### Aper√ßu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbfb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_aeronautics_data[:5])  # pour afficher un aper√ßu\n",
    "\n",
    "json_filename = f\"raw.json\"\n",
    "json_filepath = os.path.join(output_dir, json_filename)\n",
    "\n",
    "with open(json_filepath, 'w', encoding='utf-8') as jsonfile:\n",
    "    json.dump(raw_aeronautics_data, jsonfile, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5639628",
   "metadata": {},
   "source": [
    "## üìÅ Export\n",
    "\n",
    "### Construction du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da268123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è CONSTRUCTION DE LA HI√âRARCHIE FINALE\n",
    "\n",
    "# def get_id_from_uri(uri):\n",
    "#     # Ex: \"http://www.wikidata.org/entity/Q105557\" ‚Üí \"Q105557\"\n",
    "#     return uri.split(\"/\")[-1] if uri else \"\"\n",
    "\n",
    "def build_final_hierarchy(raw_aeronautics_data):\n",
    "    \"\"\"Construit la hi√©rarchie finale avec parents imm√©diats et cat√©gories\"\"\"\n",
    "    print(\"üèóÔ∏è CONSTRUCTION DE LA HI√âRARCHIE FINALE\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    # Cr√©er la hi√©rarchie structur√©e\n",
    "    hierarchy = []\n",
    "    for entry in raw_aeronautics_data:\n",
    "    \n",
    "        hierarchy.append(\n",
    "            {\n",
    "            \"ID\": clean_entity_id(entry.get(\"item\", {}).get(\"value\", \"\")),\n",
    "            \"Terme\": entry.get(\"itemLabel\", {}).get(\"value\", \"\"),\n",
    "            \"ID_TG\": clean_entity_id(entry.get(\"parent\", {}).get(\"value\", \"\")),\n",
    "            \"TG\": entry.get(\"parentLabel\", {}).get(\"value\", \"\"),\n",
    "            \"Def\": entry.get(\"itemDescription\", {}).get(\"value\", \"\"),\n",
    "            \"EP\": entry.get(\"synonyms_fr\", {}).get(\"value\", \"\"),\n",
    "            \"TA\": entry.get(\"source_category\", {})\n",
    "        }\n",
    "        )\n",
    "    \n",
    " \n",
    "    print(f\"‚úÖ Hi√©rarchie construite: {len(hierarchy)} entr√©es totales\")\n",
    "    return hierarchy\n",
    "\n",
    "final_thesaurus = build_final_hierarchy(raw_aeronautics_data)\n",
    "print(f\"üéØ Th√©saurus final: {len(final_thesaurus)} entr√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc1047",
   "metadata": {},
   "source": [
    "### Export du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc3e345",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_thesaurus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 134\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mtotal_entries\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m entr√©es de th√©saurus\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Export et r√©sum√© final\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinal_thesaurus\u001b[49m:\n\u001b[32m    135\u001b[39m     csv_file, json_file, statistics = export_final_thesaurus(final_thesaurus)\n\u001b[32m    136\u001b[39m     display_final_summary(statistics, csv_file, json_file)\n",
      "\u001b[31mNameError\u001b[39m: name 'final_thesaurus' is not defined"
     ]
    }
   ],
   "source": [
    "# üíæ EXPORT FINAL UNIQUE - CSV Occidental European Format (semicolon separated)\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def export_final_thesaurus(thesaurus_data):\n",
    "    \"\"\"Exporte le th√©saurus final en CSV (point-virgule, format europ√©en) et JSON\"\"\"\n",
    "    print(\"üíæ EXPORT FINAL UNIQUE\")\n",
    "    print(\"=\"*25)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Export CSV - Occidental European (semicolon separator, utf-8-sig BOM)\n",
    "    csv_filename = f\"thesaurus_aeronautique_FINAL_{timestamp}.csv\"\n",
    "    csv_filepath = os.path.join(output_dir, csv_filename)\n",
    "    \n",
    "    fieldnames = [\n",
    "        'ID', 'Terme', 'ID_TG','TG', 'Def', 'EP',\n",
    "        'TA'\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìÑ Export CSV: {csv_filename}\")\n",
    "    with open(csv_filepath, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        writer = csv.DictWriter(\n",
    "            csvfile, \n",
    "            fieldnames=fieldnames,\n",
    "            delimiter=';',         # Use semicolon as separator\n",
    "            quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        for entry in sorted(thesaurus_data, key=lambda x: x[\"ID\"]):\n",
    "            # Ensure all values are strings and convert None to empty string\n",
    "            row = {k: ('' if v is None else str(v)) for k, v in entry.items()}\n",
    "            # Guarantee all required fields exist in row\n",
    "            for field in fieldnames:\n",
    "                row.setdefault(field, '')\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    # 2. Export JSON avec m√©tadonn√©es\n",
    "    json_filename = f\"thesaurus_aeronautique_FINAL_{timestamp}.json\"\n",
    "    json_filepath = os.path.join(output_dir, json_filename)\n",
    "    \n",
    "    stats = analyze_thesaurus_statistics(thesaurus_data)\n",
    "    \n",
    "    json_data = {\n",
    "        \"metadata\": {\n",
    "            \"title\": \"Th√©saurus A√©ronautique Final - Wikidata\",\n",
    "            \"description\": \"Th√©saurus exhaustif avec hi√©rarchie et parents imm√©diats\",\n",
    "            \"version\": \"1.0-FINAL\",\n",
    "            \"created\": timestamp,\n",
    "            \"source\": \"Wikidata SPARQL optimis√©\",\n",
    "            \"total_entries\": len(thesaurus_data),\n",
    "            \"extraction_method\": \"multi-query_hierarchical\",\n",
    "            \"parent_detection\": \"automatic_wikidata_relations\",\n",
    "            \"multilingual_support\": True,\n",
    "            \"format\": \"structured_hierarchical_thesaurus\"\n",
    "        },\n",
    "        \"statistics\": stats,\n",
    "        \"data\": thesaurus_data\n",
    "    }\n",
    "    \n",
    "    print(f\"üìÑ Export JSON: {json_filename}\")\n",
    "    with open(json_filepath, 'w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(json_data, jsonfile, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return csv_filepath, json_filepath, stats\n",
    "\n",
    "def analyze_thesaurus_statistics(thesaurus_data):\n",
    "    \"\"\"Analyse les statistiques du th√©saurus final\"\"\"\n",
    "    stats = {\n",
    "        \"total_entries\": len(thesaurus_data),\n",
    "        \"categories\": {},\n",
    "        \"relation_types\": {},\n",
    "        \"languages\": {},\n",
    "        \"hierarchy_depth\": 0,\n",
    "        \"entries_with_synonyms\": 0,\n",
    "        \"entries_with_descriptions\": 0\n",
    "    }\n",
    "    \n",
    "    for entry in thesaurus_data:\n",
    "        # Cat√©gories\n",
    "        category = entry.get(\"category\", \"unknown\")\n",
    "        stats[\"categories\"][category] = stats[\"categories\"].get(category, 0) + 1\n",
    "        \n",
    "        # Types de relation\n",
    "        rel_type = entry.get(\"relation_type\", \"unknown\")\n",
    "        stats[\"relation_types\"][rel_type] = stats[\"relation_types\"].get(rel_type, 0) + 1\n",
    "        \n",
    "        # Langues\n",
    "        lang = entry.get(\"lang\", \"unknown\")\n",
    "        stats[\"languages\"][lang] = stats[\"languages\"].get(lang, 0) + 1\n",
    "        \n",
    "        # Enrichissements\n",
    "        if entry.get(\"synonyms\"):\n",
    "            stats[\"entries_with_synonyms\"] += 1\n",
    "        if entry.get(\"description\"):\n",
    "            stats[\"entries_with_descriptions\"] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def display_final_summary(stats, csv_file, json_file):\n",
    "    \"\"\"Affiche un r√©sum√© final du th√©saurus g√©n√©r√©\"\"\"\n",
    "    print(\"\\nüéØ R√âSUM√â FINAL DU TH√âSAURUS A√âRONAUTIQUE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"üìä STATISTIQUES G√âN√âRALES:\")\n",
    "    print(f\"   ‚Ä¢ Total d'entr√©es: {stats['total_entries']}\")\n",
    "    print(f\"   ‚Ä¢ Entr√©es avec synonymes: {stats['entries_with_synonyms']}\")\n",
    "    print(f\"   ‚Ä¢ Entr√©es avec descriptions: {stats['entries_with_descriptions']}\")\n",
    "    \n",
    "    print(f\"\\nüìÇ R√âPARTITION PAR CAT√âGORIE:\")\n",
    "    for category, count in sorted(stats[\"categories\"].items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / stats['total_entries']) * 100\n",
    "        print(f\"   ‚Ä¢ {category}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüîó TYPES DE RELATIONS:\")\n",
    "    for rel_type, count in sorted(stats[\"relation_types\"].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   ‚Ä¢ {rel_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\nüåê LANGUES:\")\n",
    "    for lang, count in stats[\"languages\"].items():\n",
    "        print(f\"   ‚Ä¢ {lang}: {count}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ FICHIERS G√âN√âR√âS:\")\n",
    "    print(f\"   ‚úÖ CSV: {os.path.basename(csv_file)}\")\n",
    "    print(f\"   ‚úÖ JSON: {os.path.basename(json_file)}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ MISSION ACCOMPLIE !\")\n",
    "    print(f\" {stats['total_entries']} entr√©es de th√©saurus\")\n",
    "\n",
    "# Export et r√©sum√© final\n",
    "if final_thesaurus:\n",
    "    csv_file, json_file, statistics = export_final_thesaurus(final_thesaurus)\n",
    "    display_final_summary(statistics, csv_file, json_file)\n",
    "else:\n",
    "    print(\"‚ùå Aucun th√©saurus √† exporter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d148f58",
   "metadata": {},
   "source": [
    "### Nettoyage des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire le CSV (remplace 'ton_fichier.csv' par le tien)\n",
    "df = pd.read_csv(csv_file, sep=';', dtype=str).fillna('')\n",
    "\n",
    "# Fonction pour concat√©ner les valeurs uniques (s√©par√©es par \"|\")\n",
    "def concat_unique(series):\n",
    "    uniques = set([v.strip() for v in series if v.strip() != ''])\n",
    "    return \" | \".join(sorted(uniques)) if uniques else ''\n",
    "\n",
    "# Grouper par 'ID', en concat√©nant les valeurs diff√©rentes pour chaque colonne\n",
    "df_clean = df.groupby('ID', as_index=False).agg(concat_unique)\n",
    "\n",
    "# Sauvegarder le r√©sultat\n",
    "df_clean.to_csv(csv_file, sep=';', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"‚úÖ CSV nettoy√© et export√© sous {csv_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
