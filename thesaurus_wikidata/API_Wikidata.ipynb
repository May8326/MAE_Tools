{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339b9f1f",
   "metadata": {},
   "source": [
    "# üöÄ Requ√™te WIKIDATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f576147d",
   "metadata": {},
   "source": [
    "## üî® Construction de l'environnement n√©cessaire et configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74111c85",
   "metadata": {},
   "source": [
    "### Installation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install SPARQLWrapper tqdm pandas\n",
    "\n",
    "print(\"‚¨áÔ∏è Installation termin√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e7bad",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86a25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ CONFIGURATION TERMIN√âE\n",
      "üìÅ Dossier de sortie: ./output\n",
      "‚è±Ô∏è  Rate limit: 3.0s entre requ√™tes\n",
      "üì¶ Taille des batches: 10\n"
     ]
    }
   ],
   "source": [
    "# üîß CONFIGURATION ET IMPORTS\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "WIKIDATA_ENDPOINT = \"https://query.wikidata.org/sparql\"\n",
    "RATE_LIMIT_DELAY = 3.0 \n",
    "BATCH_SIZE = 10  \n",
    "MAX_RETRIES = 3  \n",
    "REQUEST_TIMEOUT = 60\n",
    "ENRICHMENT_BATCH_SIZE = 15\n",
    "LOOP_LIMIT = 25\n",
    "LOOP_OFFSET = 0\n",
    "\n",
    "# Dossier de sortie\n",
    "output_dir = \"./output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"üöÄ CONFIGURATION TERMIN√âE\")\n",
    "print(f\"üìÅ Dossier de sortie: {output_dir}\")\n",
    "print(f\"‚è±Ô∏è  Rate limit: {RATE_LIMIT_DELAY}s entre requ√™tes\")\n",
    "print(f\"üì¶ Taille des batches: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519640a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó FONCTIONS CONFIGUR√âES :\n",
      "      - create_sparql_client: Cr√©e un client SPARQL pour Wikidata\n",
      "      - execute_sparql_query: Ex√©cute une requ√™te SPARQL avec gestion\n",
      "        des erreurs, rate limiting et pagination optionnelle\n",
      "        - clean_entity_id: Extrait l'ID d'une entit√© √† partir de son URI\n",
      "        - execute_batch_queries: Ex√©cute une liste de requ√™tes SPARQL en batch\n",
      "        - execute_paginated_query: Ex√©cute une requ√™te SPARQL avec pagination\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "def create_sparql_client():\n",
    "    \"\"\"\n",
    "    Cr√©er un client SPARQL pour interagir avec Wikidata\n",
    "    :return: Instance de SPARQLWrapper configur√©e pour Wikidata\n",
    "    \"\"\"\n",
    "    sparql = SPARQLWrapper(WIKIDATA_ENDPOINT)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.setTimeout(REQUEST_TIMEOUT)\n",
    "    return sparql\n",
    "\n",
    "def execute_sparql_query(query, max_retries=MAX_RETRIES, use_pagination=False, limit=None, max_results=None):\n",
    "    \"\"\"\n",
    "    Ex√©cute une requ√™te SPARQL avec gestion des erreurs, rate limiting et pagination optionnelle\n",
    "    :param query: La requ√™te SPARQL √† ex√©cuter\n",
    "    :param max_retries: Nombre maximum de tentatives en cas d'√©chec\n",
    "    :param use_pagination: Si True, active la pagination automatique\n",
    "    :param limit: Taille des pages pour la pagination (d√©faut: LOOP_LIMIT)\n",
    "    :param max_results: Nombre maximum de r√©sultats √† r√©cup√©rer (None = illimit√©)\n",
    "    :return: R√©sultats de la requ√™te ou une liste vide en cas d'√©chec\n",
    "    \"\"\"\n",
    "    sparql = create_sparql_client()\n",
    "    \n",
    "    # Mode simple sans pagination\n",
    "    if not use_pagination:\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                sparql.setQuery(query)\n",
    "                results = sparql.query().convert()\n",
    "                time.sleep(RATE_LIMIT_DELAY)\n",
    "                return results[\"results\"][\"bindings\"]\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Tentative {attempt + 1}/{max_retries} √©chou√©e: {e}...\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(RATE_LIMIT_DELAY * (attempt + 2))\n",
    "                else:\n",
    "                    print(f\"‚ùå Requ√™te √©chou√©e apr√®s {max_retries} tentatives\")\n",
    "                    return []\n",
    "        return []\n",
    "    \n",
    "    # Mode pagination\n",
    "    if limit is None:\n",
    "        limit = LOOP_LIMIT\n",
    "    \n",
    "    all_results = []\n",
    "    offset = 0\n",
    "    \n",
    "    print(f\"üîç D√©but de la pagination (limit={limit})...\")\n",
    "    \n",
    "    while True:\n",
    "        # Ajouter LIMIT et OFFSET √† la requ√™te\n",
    "        paginated_query = f\"{query.rstrip()} LIMIT {limit} OFFSET {offset}\"\n",
    "        \n",
    "        print(f\"üîπ Requ√™te OFFSET {offset}, LIMIT {limit}\")\n",
    "        \n",
    "        success = False\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                sparql.setQuery(paginated_query)\n",
    "                results = sparql.query().convert()\n",
    "                bindings = results[\"results\"][\"bindings\"]\n",
    "                \n",
    "                if not bindings:\n",
    "                    print(\"‚úÖ Fin de la pagination - Aucun r√©sultat suppl√©mentaire.\")\n",
    "                    success = True\n",
    "                    break\n",
    "                \n",
    "                all_results.extend(bindings)\n",
    "                print(f\"‚úÖ R√©cup√©r√© {len(bindings)} r√©sultats (total: {len(all_results)})\")\n",
    "                \n",
    "                # V√©rifier la limite max_results\n",
    "                if max_results and len(all_results) >= max_results:\n",
    "                    print(f\"üéØ Limite de {max_results} r√©sultats atteinte.\")\n",
    "                    all_results = all_results[:max_results]  # Tronquer si n√©cessaire\n",
    "                    success = True\n",
    "                    break\n",
    "                \n",
    "                success = True\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Tentative {attempt + 1}/{max_retries} √©chou√©e √† l'offset {offset}: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(RATE_LIMIT_DELAY * (attempt + 2))\n",
    "                else:\n",
    "                    print(f\"‚ùå Requ√™te √©chou√©e apr√®s {max_retries} tentatives √† l'offset {offset}\")\n",
    "                    return all_results  # Retourner ce qu'on a r√©ussi √† r√©cup√©rer\n",
    "        \n",
    "        if not success or (max_results and len(all_results) >= max_results):\n",
    "            break\n",
    "        \n",
    "        # Incr√©menter l'offset pour la prochaine page\n",
    "        offset += limit\n",
    "        \n",
    "        # Rate limiting crucial\n",
    "        print(f\"‚è≥ Attente de {RATE_LIMIT_DELAY}s...\")\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    print(f\"üéØ Total final r√©cup√©r√© : {len(all_results)} r√©sultats.\")\n",
    "    return all_results\n",
    "\n",
    "def clean_entity_id(entity_uri):\n",
    "    \"\"\"\n",
    "    Extrait l'ID d'une entit√© √† partir de son URI\n",
    "    :param entity_uri: URI de l'entit√© (ex: \"http://www.wikidata.org/entity/Q42'\")\n",
    "    :return: ID de l'entit√© (ex: \"Q42\") ou une cha√Æne vide si l'URI est vide\n",
    "    \"\"\"\n",
    "    if not entity_uri:\n",
    "        return \"\"\n",
    "    return entity_uri.split(\"/\")[-1] if \"/\" in entity_uri else entity_uri\n",
    "\n",
    "def execute_batch_queries(queries, description=\"Requ√™tes\", use_pagination=False):\n",
    "    \"\"\"\n",
    "    Ex√©cute une liste de requ√™tes SPARQL en batch\n",
    "    :param queries: Liste de requ√™tes SPARQL √† ex√©cuter\n",
    "    :param description: Description de la t√¢che pour l'affichage\n",
    "    :param use_pagination: Active la pagination pour chaque requ√™te\n",
    "    :return: Liste de tous les r√©sultats combin√©s\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    for i, query in enumerate(tqdm(queries, desc=description)):\n",
    "        results = execute_sparql_query(query, use_pagination=use_pagination)\n",
    "        all_results.extend(results)\n",
    "        if (i + 1) % BATCH_SIZE == 0:\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "    return all_results\n",
    "\n",
    "def execute_paginated_query(base_query, limit=None, max_results=None):\n",
    "    \"\"\"\n",
    "    Fonction helper pour ex√©cuter facilement une requ√™te avec pagination\n",
    "    :param base_query: Requ√™te SPARQL de base (sans LIMIT/OFFSET)\n",
    "    :param limit: Taille des pages (d√©faut: LOOP_LIMIT)\n",
    "    :param max_results: Nombre maximum de r√©sultats (None = illimit√©)\n",
    "    :return: Liste de tous les r√©sultats\n",
    "    \"\"\"\n",
    "    return execute_sparql_query(\n",
    "        base_query, \n",
    "        use_pagination=True, \n",
    "        limit=limit, \n",
    "        max_results=max_results\n",
    "    )\n",
    "\n",
    "print(\"\"\"üîó FONCTIONS CONFIGUR√âES :\n",
    "      - create_sparql_client: Cr√©e un client SPARQL pour Wikidata\n",
    "      - execute_sparql_query: Ex√©cute une requ√™te SPARQL avec gestion\n",
    "        des erreurs, rate limiting et pagination optionnelle\n",
    "        - clean_entity_id: Extrait l'ID d'une entit√© √† partir de son URI\n",
    "        - execute_batch_queries: Ex√©cute une liste de requ√™tes SPARQL en batch\n",
    "        - execute_paginated_query: Ex√©cute une requ√™te SPARQL avec pagination\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d392e0",
   "metadata": {},
   "source": [
    "## üöß Construction de la requ√™te\n",
    "\n",
    "\n",
    "### Rechercher une entit√© par nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c027626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ IDENTIFICATION DE L'ENTIT√â\n",
    "\n",
    "def find_aeronautics_entity():\n",
    "    \"\"\"\n",
    "    Demande √† l'utilisateur le nom de l'entit√© √† rechercher dans Wikidata\n",
    "    :return: Tuple contenant l'ID et le label de l'entit√©\n",
    "    \"\"\"\n",
    "    entity_name = input(\"Entrez le nom de l'entit√© √† rechercher dans wikidata, en anglais (ex: aeronautics) : \").strip()\n",
    "    if not entity_name:\n",
    "        print(\"‚ùå Aucun nom d'entit√© fourni.\")\n",
    "        return None, None  # ‚úÖ Coh√©rent avec l'assignation\n",
    "\n",
    "    # Choix de la requ√™te √† utiliser\n",
    "    query_choice = input(\n",
    "        \"Quel type de requ√™te utiliser ?\\n\"\n",
    "        \"1Ô∏è‚É£ Recherche d'une cha√Æne dans les labels\\n\"\n",
    "        \"2Ô∏è‚É£ Requ√™te d'une cha√Æne dans les parents\\n\"\n",
    "        \"Entrez le num√©ro de votre choix (1 ou 2) : \"\n",
    "    ).strip()\n",
    "\n",
    "    if query_choice == \"1\":\n",
    "        query_regex = f\"\"\"\n",
    "        SELECT ?item ?itemLabel\n",
    "        WHERE {{\n",
    "            ?item rdfs:label ?itemLabel.\n",
    "            FILTER(LANG(?itemLabel) = \"en\").\n",
    "            FILTER(CONTAINS(LCASE(?itemLabel), \"{entity_name}\")).\n",
    "        }}\n",
    "        \"\"\"\n",
    "    elif query_choice == \"2\":\n",
    "        query_regex = f\"\"\"\n",
    "        SELECT ?item ?itemLabel\n",
    "        WHERE {{\n",
    "            ?item wdt:P31*/wdt:P279* ?parent.\n",
    "            ?parent rdfs:label ?parentLabel.\n",
    "            FILTER(LANG(?parentLabel) = \"en\").\n",
    "            FILTER(CONTAINS(LCASE(?parentLabel), \"{entity_name}\")).\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        print(\"‚ùå Choix invalide. Veuillez entrer 1 ou 2.\")\n",
    "        return None, None  # ‚úÖ Coh√©rent avec l'assignation\n",
    "\n",
    "    print(f\"üîç Recherche de l'entit√© principale '{entity_name}'...\")\n",
    "    results = execute_paginated_query(query_regex)  # Limite pour voir plusieurs r√©sultats\n",
    "\n",
    "    if results:\n",
    "        print(f\"\\nüìã {len(results)} entit√©s trouv√©es:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Afficher les premiers r√©sultats pour que l'utilisateur puisse voir\n",
    "        for i, entity in enumerate(results[:5]):\n",
    "            entity_id = clean_entity_id(entity[\"item\"][\"value\"])\n",
    "            entity_label = entity[\"itemLabel\"][\"value\"]\n",
    "            print(f\"{i+1}. {entity_label} ({entity_id})\")\n",
    "        \n",
    "        if len(results) > 5:\n",
    "            print(f\"... et {len(results) - 5} autres r√©sultats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b2196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonctions d'ex√©cution g√©n√©rique configur√©es:\n",
      "   ‚Ä¢ execute_and_save_query(): Ex√©cute et sauvegarde une requ√™te\n",
      "   ‚Ä¢ execute_custom_query(): Interface interactive pour requ√™tes personnalis√©es\n"
     ]
    }
   ],
   "source": [
    "def execute_and_save_query(query, filename=None, limit=None, max_results=None, description=\"Requ√™te SPARQL\"):\n",
    "    \"\"\"\n",
    "    Ex√©cute une requ√™te SPARQL avec pagination et sauvegarde automatiquement en JSON\n",
    "    :param query: Requ√™te SPARQL √† ex√©cuter\n",
    "    :param filename: Nom du fichier JSON (sans extension). Si None, g√©n√®re automatiquement\n",
    "    :param limit: Taille des pages pour la pagination (d√©faut: LOOP_LIMIT)\n",
    "    :param max_results: Nombre maximum de r√©sultats (None = illimit√©)\n",
    "    :param description: Description pour les logs\n",
    "    :return: Tuple (results, json_filepath) - R√©sultats et chemin du fichier JSON\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ EX√âCUTION: {description}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Ex√©cuter la requ√™te avec pagination\n",
    "    print(f\"üîç Ex√©cution de la requ√™te...\")\n",
    "    results = execute_paginated_query(query, limit=limit, max_results=max_results)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå Aucun r√©sultat trouv√©\")\n",
    "        return [], None\n",
    "    \n",
    "    print(f\"‚úÖ {len(results)} r√©sultats r√©cup√©r√©s\")\n",
    "    \n",
    "    # G√©n√©rer le nom de fichier si non fourni\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"query_results_{timestamp}\"\n",
    "    \n",
    "    # Ajouter l'extension .json si elle n'est pas pr√©sente\n",
    "    if not filename.endswith('.json'):\n",
    "        filename += '.json'\n",
    "    \n",
    "    # Chemin complet du fichier\n",
    "    json_filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Cr√©er les donn√©es √† sauvegarder avec m√©tadonn√©es\n",
    "    data_to_save = {\n",
    "        \"metadata\": {\n",
    "            \"description\": description,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"total_results\": len(results),\n",
    "            \"query\": query.strip(),\n",
    "            \"pagination_limit\": limit or LOOP_LIMIT,\n",
    "            \"max_results\": max_results\n",
    "        },\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    # Sauvegarder en JSON\n",
    "    try:\n",
    "        with open(json_filepath, 'w', encoding='utf-8') as jsonfile:\n",
    "            json.dump(data_to_save, jsonfile, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"üíæ R√©sultats sauvegard√©s: {filename}\")\n",
    "        print(f\"üìÅ Chemin: {json_filepath}\")\n",
    "        \n",
    "        # Afficher un aper√ßu des r√©sultats\n",
    "        print(f\"\\nüìä APER√áU DES R√âSULTATS:\")\n",
    "        for i, result in enumerate(results[:3]):\n",
    "            print(f\"üîπ R√©sultat {i+1}:\")\n",
    "            for key, value in result.items():\n",
    "                if isinstance(value, dict) and 'value' in value:\n",
    "                    print(f\"   ‚Ä¢ {key}: {value['value'][:100]}...\")\n",
    "                else:\n",
    "                    print(f\"   ‚Ä¢ {key}: {str(value)[:100]}...\")\n",
    "            print()\n",
    "        \n",
    "        if len(results) > 3:\n",
    "            print(f\"... et {len(results) - 3} autres r√©sultats\")\n",
    "        \n",
    "        return results, json_filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la sauvegarde: {e}\")\n",
    "        return results, None\n",
    "\n",
    "def execute_custom_query():\n",
    "    \"\"\"\n",
    "    Fonction interactive pour ex√©cuter une requ√™te personnalis√©e\n",
    "    \"\"\"\n",
    "    print(\"üéØ EX√âCUTION D'UNE REQU√äTE PERSONNALIS√âE\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    # Demander la requ√™te √† l'utilisateur\n",
    "    print(\"üìù Entrez votre requ√™te SPARQL (tapez 'END' sur une ligne vide pour terminer):\")\n",
    "    query_lines = []\n",
    "    while True:\n",
    "        line = input()\n",
    "        if line.strip().upper() == 'END':\n",
    "            break\n",
    "        query_lines.append(line)\n",
    "    \n",
    "    query = '\\n'.join(query_lines)\n",
    "    \n",
    "    if not query.strip():\n",
    "        print(\"‚ùå Requ√™te vide\")\n",
    "        return None, None\n",
    "    \n",
    "    # Demander les param√®tres optionnels\n",
    "    description = input(\"üìã Description de la requ√™te (optionnel): \").strip()\n",
    "    if not description:\n",
    "        description = \"Requ√™te personnalis√©e\"\n",
    "    \n",
    "    filename = input(\"üìÅ Nom du fichier de sauvegarde (optionnel, sans extension): \").strip()\n",
    "    if not filename:\n",
    "        filename = None\n",
    "    \n",
    "    limit_input = input(f\"üìä Limite par page (d√©faut: {LOOP_LIMIT}): \").strip()\n",
    "    limit = int(limit_input) if limit_input.isdigit() else None\n",
    "    \n",
    "    max_results_input = input(\"üéØ Nombre maximum de r√©sultats (optionnel): \").strip()\n",
    "    max_results = int(max_results_input) if max_results_input.isdigit() else None\n",
    "    \n",
    "    # Ex√©cuter la requ√™te\n",
    "    return execute_and_save_query(\n",
    "        query=query,\n",
    "        filename=filename,\n",
    "        limit=limit,\n",
    "        max_results=max_results,\n",
    "        description=description\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Fonctions d'ex√©cution g√©n√©rique configur√©es:\")\n",
    "print(\"   ‚Ä¢ execute_and_save_query(): Ex√©cute et sauvegarde une requ√™te\")\n",
    "print(\"   ‚Ä¢ execute_custom_query(): Interface interactive pour requ√™tes personnalis√©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e118180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Recherche de l'entit√© principale 'gallifrey'...\n",
      "üîç Requ√™te s√©lectionn√©e:\n",
      "        SELECT ?item ?itemLabel\n",
      "        WHERE {\n",
      "            ?item rdfs:label ?itemLabel.\n",
      "            FILTER(LANG(?itemLabel) = \"en\").\n",
      "            FILTER(CONTAINS(LCASE(?itemLabel), \"gallifrey\")).\n",
      "        }\n",
      "        \n",
      "üöÄ EX√âCUTION: Requ√™te SPARQL\n",
      "==================================================\n",
      "üîç Ex√©cution de la requ√™te...\n",
      "üîç D√©but de la pagination (limit=25)...\n",
      "üîπ Requ√™te OFFSET 0, LIMIT 25\n",
      "‚ö†Ô∏è  Tentative 1/3 √©chou√©e √† l'offset 0: The read operation timed out\n",
      "‚ö†Ô∏è  Tentative 2/3 √©chou√©e √† l'offset 0: The read operation timed out\n",
      "‚ö†Ô∏è  Tentative 3/3 √©chou√©e √† l'offset 0: The read operation timed out\n",
      "‚ùå Requ√™te √©chou√©e apr√®s 3 tentatives √† l'offset 0\n",
      "‚ùå Aucun r√©sultat trouv√©\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# üéØ IDENTIFICATION DE L'ENTIT√â\n",
    "\n",
    "def find_aeronautics_entity():\n",
    "    \"\"\"\n",
    "    Demande √† l'utilisateur le nom de l'entit√© √† rechercher dans Wikidata\n",
    "    :return: Tuple contenant l'ID et le label de l'entit√©\n",
    "    \"\"\"\n",
    "    entity_name = input(\"Entrez le nom de l'entit√© √† rechercher dans wikidata, en anglais (ex: aeronautics) : \").strip()\n",
    "    if not entity_name:\n",
    "        print(\"‚ùå Aucun nom d'entit√© fourni.\")\n",
    "        return None, None  # ‚úÖ Coh√©rent avec l'assignation\n",
    "\n",
    "    # Choix de la requ√™te √† utiliser\n",
    "    query_choice = input(\n",
    "        \"Quel type de requ√™te utiliser ?\\n\"\n",
    "        \"1Ô∏è‚É£ Recherche d'une cha√Æne dans les labels\\n\"\n",
    "        \"2Ô∏è‚É£ Requ√™te d'une cha√Æne dans les parents\\n\"\n",
    "        \"Entrez le num√©ro de votre choix (1 ou 2) : \"\n",
    "    ).strip()\n",
    "\n",
    "    if query_choice == \"1\":\n",
    "        query_regex = f\"\"\"\n",
    "        SELECT ?item ?itemLabel\n",
    "        WHERE {{\n",
    "            ?item rdfs:label ?itemLabel.\n",
    "            FILTER(LANG(?itemLabel) = \"en\").\n",
    "            FILTER(CONTAINS(LCASE(?itemLabel), \"{entity_name}\")).\n",
    "        }}\n",
    "        \"\"\"\n",
    "    elif query_choice == \"2\":\n",
    "        query_regex = f\"\"\"\n",
    "        SELECT ?item ?itemLabel\n",
    "        WHERE {{\n",
    "            ?item wdt:P31*/wdt:P279* ?parent.\n",
    "            ?parent rdfs:label ?parentLabel.\n",
    "            FILTER(LANG(?parentLabel) = \"en\").\n",
    "            FILTER(CONTAINS(LCASE(?parentLabel), \"{entity_name}\")).\n",
    "            SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        print(\"‚ùå Choix invalide. Veuillez entrer 1 ou 2.\")\n",
    "        return None, None  # ‚úÖ Coh√©rent avec l'assignation\n",
    "\n",
    "    print(f\"üîç Recherche de l'entit√© principale '{entity_name}'...\")\n",
    "    print(f\"üîç Requ√™te s√©lectionn√©e:{query_regex}\")\n",
    "    return execute_and_save_query(query_regex)\n",
    "find_aeronautics_entity()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020b073",
   "metadata": {},
   "source": [
    "### Requ√™tes SPARQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "295aa2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Requ√™tes d√©finies (avec synonymes fran√ßais)\n"
     ]
    }
   ],
   "source": [
    "def build_aeronautics_extraction_queries():\n",
    "    \"\"\"Construit les requ√™tes d'extraction des donn√©es\"\"\"\n",
    "    queries = {\n",
    "        \"manufacturers\": \"\"\"\n",
    "        SELECT DISTINCT ?item \n",
    "              \n",
    "                ?itemLabel\n",
    "               (COALESCE(?itemDescription_fr, ?itemDescription_en, ?itemDescription_any, \"\") AS ?itemDescription)\n",
    "               ?parent\n",
    "               ?parentLabel\n",
    "               (GROUP_CONCAT(DISTINCT ?synonym_fr; separator=\",\") AS ?synonyms_fr)\n",
    "        WHERE {\n",
    "          # Tous les √©quipements d'aviation (instances ou sous-classes ou parties)\n",
    "          ?item wdt:P31*/wdt:P279*/wdt:P361*/wdt:P452*/wdt:P749* wd:Q936518 .\n",
    "        \n",
    "          # On cherche le parent imm√©diat selon diff√©rentes relations\n",
    "            OPTIONAL { ?item wdt:P361 ?partOf . }\n",
    "            OPTIONAL { ?item wdt:P279 ?subclassOf . }\n",
    "            OPTIONAL { ?item wdt:P31 ?instanceOf . }\n",
    "            OPTIONAL { ?item wdt:P452 ?secteur .}\n",
    "            OPTIONAL { ?item wdt:P176 ?constructeur.}\n",
    "            OPTIONAL { ?item wdt:P749 ?constructeur.}\n",
    "            \n",
    "            BIND(COALESCE(?partOf, ?subclassOf, ?instanceOf, ?secteur, ?constructeur) AS ?parent)\n",
    "        \n",
    "          # Description de l'item (fr > en > autre)\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_fr . FILTER(LANG(?itemDescription_fr) = \"fr\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_en . FILTER(LANG(?itemDescription_en) = \"en\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_any .\n",
    "                     FILTER(LANG(?itemDescription_any) != \"fr\" && LANG(?itemDescription_any) != \"en\") }\n",
    "\n",
    "          # Synonymes en fran√ßais (P1709 est \"synonymes exacts\")\n",
    "          OPTIONAL { ?item skos:altLabel ?synonym_fr . FILTER(LANG(?synonym_fr) = \"fr\") }\n",
    "        \n",
    "          SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language \"fr,en,[AUTO_LANGUAGE]\"\n",
    "          }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel ?itemDescription_fr ?itemDescription_en ?itemDescription_any ?parent ?parentLabel\n",
    "        \"\"\",\n",
    "\n",
    "        \"aircraft_models\": \"\"\"\n",
    "        SELECT DISTINCT ?item \n",
    "               ?itemLabel\n",
    "               (COALESCE(?itemDescription_fr, ?itemDescription_en, ?itemDescription_any, \"\") AS ?itemDescription)\n",
    "               ?parent\n",
    "               ?parentLabel\n",
    "               (GROUP_CONCAT(DISTINCT ?synonym_fr; separator=\" , \") AS ?synonyms_fr)\n",
    "        WHERE {\n",
    "          # Tous les mod√®les d'avions (instances ou sous-classes ou parties)\n",
    "          ?item wdt:P31/wdt:P279* wd:Q11436 .\n",
    "          \n",
    "        \n",
    "          # On cherche le parent imm√©diat selon diff√©rentes relations         \n",
    "            OPTIONAL { ?item wdt:P179 ?series .}\n",
    "            OPTIONAL { ?item wdt:176 ?constructeur.}\n",
    "            OPTIONAL { ?item wdt:P31 ?instanceOf . } \n",
    "            OPTIONAL { ?item wdt:P361 ?partOf . }\n",
    "            OPTIONAL { ?item wdt:P279 ?subclassOf . }\n",
    "                       \n",
    "            BIND(COALESCE(?partOf, ?subclassOf, ?instanceOf, ?series, ?constructeur) AS ?parent)\n",
    "        \n",
    "          # Description de l'item (fr > en > autre)\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_fr . FILTER(LANG(?itemDescription_fr) = \"fr\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_en . FILTER(LANG(?itemDescription_en) = \"en\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_any .\n",
    "                     FILTER(LANG(?itemDescription_any) != \"fr\" && LANG(?itemDescription_any) != \"en\") }\n",
    "\n",
    "          # Synonymes en fran√ßais\n",
    "          OPTIONAL { ?item skos:altLabel ?synonym_fr . FILTER(LANG(?synonym_fr) = \"fr\") }\n",
    "        \n",
    "          SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language \"fr,en,[AUTO_LANGUAGE]\"\n",
    "          }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel ?itemDescription_fr ?itemDescription_en ?itemDescription_any ?parent ?parentLabel\n",
    "        \"\"\",\n",
    "\n",
    "        \"aircraft_components\": \"\"\"\n",
    "        SELECT DISTINCT ?item \n",
    "               ?itemLabel\n",
    "               (COALESCE(?itemDescription_fr, ?itemDescription_en, ?itemDescription_any, \"\") AS ?itemDescription)\n",
    "               ?parent\n",
    "               ?parentLabel\n",
    "               (GROUP_CONCAT(DISTINCT ?synonym_fr; separator=\" , \") AS ?synonyms_fr)\n",
    "        WHERE {\n",
    "          # Tous les √©quipements d'aviation (instances ou sous-classes ou parties)\n",
    "          ?item wdt:P31*/wdt:P279*/wdt:P361* wd:Q16693356 .\n",
    "        \n",
    "          # On cherche le parent imm√©diat selon diff√©rentes relations\n",
    "            OPTIONAL { ?item wdt:P361 ?partOf . }\n",
    "            OPTIONAL { ?item wdt:P279 ?subclassOf . }\n",
    "            OPTIONAL { ?item wdt:P31 ?instanceOf . }\n",
    "            OPTIONAL { ?item wdt:P452 ?secteur .}\n",
    "            OPTIONAL { ?item wdt:P176 ?constructeur.}\n",
    "            \n",
    "            BIND(COALESCE(?partOf, ?subclassOf, ?instanceOf, ?secteur, ?constructeur) AS ?parent)\n",
    "        \n",
    "          # Description de l'item (fr > en > autre)\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_fr . FILTER(LANG(?itemDescription_fr) = \"fr\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_en . FILTER(LANG(?itemDescription_en) = \"en\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_any .\n",
    "                     FILTER(LANG(?itemDescription_any) != \"fr\" && LANG(?itemDescription_any) != \"en\") }\n",
    "\n",
    "          # Synonymes en fran√ßais\n",
    "          OPTIONAL { ?item skos:altLabel ?synonym_fr . FILTER(LANG(?synonym_fr) = \"fr\") }\n",
    "        \n",
    "          SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language \"fr,en,[AUTO_LANGUAGE]\"\n",
    "          }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel ?itemDescription_fr ?itemDescription_en ?itemDescription_any ?parent ?parentLabel\n",
    "        \"\"\",\n",
    "\n",
    "        \"aeronautic_profession\": \"\"\"\n",
    "        SELECT DISTINCT ?item \n",
    "               ?itemLabel\n",
    "               (COALESCE(?itemDescription_fr, ?itemDescription_en, ?itemDescription_any, \"\") AS ?itemDescription)\n",
    "               ?parent\n",
    "               ?parentLabel\n",
    "               (GROUP_CONCAT(DISTINCT ?synonym_fr; separator=\" , \") AS ?synonyms_fr)\n",
    "        WHERE {\n",
    "        ?item wdt:P425* ?domaine.\n",
    "        VALUES ?domaine { wd:Q765633 wd:Q906438 wd:Q1434048 wd:Q206814 wd:Q627716 wd:Q221395 wd:Q765633 wd:Q22719}.  \n",
    "        \n",
    "          # On cherche le parent imm√©diat selon diff√©rentes relations\n",
    "            OPTIONAL { ?item wdt:P361 ?partOf . }\n",
    "            OPTIONAL { ?item wdt:P279 ?subclassOf . }\n",
    "            OPTIONAL { ?item wdt:P31 ?instanceOf . }\n",
    "            OPTIONAL { ?item wdt:P452 ?secteur .}\n",
    "            OPTIONAL { ?item wdt:P176 ?constructeur.}\n",
    "            OPTIONAL { ?item wdt:P749 ?constructeur.}\n",
    "            \n",
    "            BIND(COALESCE(?partOf, ?subclassOf, ?instanceOf, ?secteur, ?constructeur, ?domaine) AS ?parent) # Attention √† coalesce pour √©viter les doublons\n",
    "        \n",
    "          # Description de l'item (fr > en > autre)\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_fr . FILTER(LANG(?itemDescription_fr) = \"fr\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_en . FILTER(LANG(?itemDescription_en) = \"en\") }\n",
    "          OPTIONAL { ?item schema:description ?itemDescription_any .\n",
    "                     FILTER(LANG(?itemDescription_any) != \"fr\" && LANG(?itemDescription_any) != \"en\") }\n",
    "\n",
    "          # Synonymes en fran√ßais\n",
    "          OPTIONAL { ?item skos:altLabel ?synonym_fr . FILTER(LANG(?synonym_fr) = \"fr\") }\n",
    "        \n",
    "          SERVICE wikibase:label {\n",
    "            bd:serviceParam wikibase:language \"fr,en,[AUTO_LANGUAGE]\"\n",
    "          }\n",
    "        }\n",
    "        GROUP BY ?item ?itemLabel ?itemDescription_fr ?itemDescription_en ?itemDescription_any ?parent ?parentLabel\n",
    "        \"\"\"\n",
    "    }\n",
    "    return queries\n",
    "\n",
    "print(\"‚úÖ Requ√™tes d√©finies (avec synonymes fran√ßais)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea367aee",
   "metadata": {},
   "source": [
    "## üîé Lancer la recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b6ebe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è EXTRACTION HI√âRARCHIQUE EXHAUSTIVE\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_aeronautics_extraction_queries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéØ TOTAL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m entit√©s extraites\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_results\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m raw_aeronautics_data = \u001b[43mextract_all_aeronautics_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mextract_all_aeronautics_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müèóÔ∏è EXTRACTION HI√âRARCHIQUE EXHAUSTIVE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m queries = \u001b[43mbuild_aeronautics_extraction_queries\u001b[49m()\n\u001b[32m      7\u001b[39m all_results = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m category, query \u001b[38;5;129;01min\u001b[39;00m queries.items():\n",
      "\u001b[31mNameError\u001b[39m: name 'build_aeronautics_extraction_queries' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_all_aeronautics_data():\n",
    "    \"\"\"Extrait toutes les donn√©es a√©ronautiques de mani√®re optimis√©e\"\"\"\n",
    "    print(\"üèóÔ∏è EXTRACTION HI√âRARCHIQUE EXHAUSTIVE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    queries = build_aeronautics_extraction_queries()\n",
    "    all_results = []\n",
    "    \n",
    "    for category, query in queries.items():\n",
    "        print(f\"\\nüîç Extraction: {category}\")\n",
    "        results = execute_sparql_query(query)\n",
    "        \n",
    "        # Enrichir chaque r√©sultat avec sa cat√©gorie\n",
    "        for result in results:\n",
    "            result[\"source_category\"] = category\n",
    "        \n",
    "        all_results.extend(results)\n",
    "        print(f\"‚úÖ {len(results)} entit√©s trouv√©es pour {category}\")\n",
    "    \n",
    "    print(f\"\\nüéØ TOTAL: {len(all_results)} entit√©s extraites\")\n",
    "    return all_results\n",
    "\n",
    "raw_aeronautics_data = extract_all_aeronautics_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dffabe",
   "metadata": {},
   "source": [
    "### Aper√ßu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85bbfb4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_aeronautics_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mraw_aeronautics_data\u001b[49m[:\u001b[32m5\u001b[39m])  \u001b[38;5;66;03m# pour afficher un aper√ßu\u001b[39;00m\n\u001b[32m      3\u001b[39m json_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mraw.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m json_filepath = os.path.join(output_dir, json_filename)\n",
      "\u001b[31mNameError\u001b[39m: name 'raw_aeronautics_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(raw_aeronautics_data[:5])  # pour afficher un aper√ßu\n",
    "\n",
    "json_filename = f\"raw.json\"\n",
    "json_filepath = os.path.join(output_dir, json_filename)\n",
    "\n",
    "with open(json_filepath, 'w', encoding='utf-8') as jsonfile:\n",
    "    json.dump(raw_aeronautics_data, jsonfile, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5639628",
   "metadata": {},
   "source": [
    "## üìÅ Export\n",
    "\n",
    "### Construction du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da268123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è CONSTRUCTION DE LA HI√âRARCHIE FINALE\n",
    "\n",
    "# def get_id_from_uri(uri):\n",
    "#     # Ex: \"http://www.wikidata.org/entity/Q105557\" ‚Üí \"Q105557\"\n",
    "#     return uri.split(\"/\")[-1] if uri else \"\"\n",
    "\n",
    "def build_final_hierarchy(raw_aeronautics_data):\n",
    "    \"\"\"Construit la hi√©rarchie finale avec parents imm√©diats et cat√©gories\"\"\"\n",
    "    print(\"üèóÔ∏è CONSTRUCTION DE LA HI√âRARCHIE FINALE\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    # Cr√©er la hi√©rarchie structur√©e\n",
    "    hierarchy = []\n",
    "    for entry in raw_aeronautics_data:\n",
    "    \n",
    "        hierarchy.append(\n",
    "            {\n",
    "            \"ID\": clean_entity_id(entry.get(\"item\", {}).get(\"value\", \"\")),\n",
    "            \"Terme\": entry.get(\"itemLabel\", {}).get(\"value\", \"\"),\n",
    "            \"ID_TG\": clean_entity_id(entry.get(\"parent\", {}).get(\"value\", \"\")),\n",
    "            \"TG\": entry.get(\"parentLabel\", {}).get(\"value\", \"\"),\n",
    "            \"Def\": entry.get(\"itemDescription\", {}).get(\"value\", \"\"),\n",
    "            \"EP\": entry.get(\"synonyms_fr\", {}).get(\"value\", \"\"),\n",
    "            \"TA\": entry.get(\"source_category\", {})\n",
    "        }\n",
    "        )\n",
    "    \n",
    " \n",
    "    print(f\"‚úÖ Hi√©rarchie construite: {len(hierarchy)} entr√©es totales\")\n",
    "    return hierarchy\n",
    "\n",
    "final_thesaurus = build_final_hierarchy(raw_aeronautics_data)\n",
    "print(f\"üéØ Th√©saurus final: {len(final_thesaurus)} entr√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc1047",
   "metadata": {},
   "source": [
    "### Export du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ EXPORT FINAL UNIQUE - CSV Occidental European Format (semicolon separated)\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def export_final_thesaurus(thesaurus_data):\n",
    "    \"\"\"Exporte le th√©saurus final en CSV (point-virgule, format europ√©en) et JSON\"\"\"\n",
    "    print(\"üíæ EXPORT FINAL UNIQUE\")\n",
    "    print(\"=\"*25)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Export CSV - Occidental European (semicolon separator, utf-8-sig BOM)\n",
    "    csv_filename = f\"thesaurus_aeronautique_FINAL_{timestamp}.csv\"\n",
    "    csv_filepath = os.path.join(output_dir, csv_filename)\n",
    "    \n",
    "    fieldnames = [\n",
    "        'ID', 'Terme', 'ID_TG','TG', 'Def', 'EP',\n",
    "        'TA'\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìÑ Export CSV: {csv_filename}\")\n",
    "    with open(csv_filepath, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        writer = csv.DictWriter(\n",
    "            csvfile, \n",
    "            fieldnames=fieldnames,\n",
    "            delimiter=';',         # Use semicolon as separator\n",
    "            quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        for entry in sorted(thesaurus_data, key=lambda x: x[\"ID\"]):\n",
    "            # Ensure all values are strings and convert None to empty string\n",
    "            row = {k: ('' if v is None else str(v)) for k, v in entry.items()}\n",
    "            # Guarantee all required fields exist in row\n",
    "            for field in fieldnames:\n",
    "                row.setdefault(field, '')\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    # 2. Export JSON avec m√©tadonn√©es\n",
    "    json_filename = f\"thesaurus_aeronautique_FINAL_{timestamp}.json\"\n",
    "    json_filepath = os.path.join(output_dir, json_filename)\n",
    "    \n",
    "    stats = analyze_thesaurus_statistics(thesaurus_data)\n",
    "    \n",
    "    json_data = {\n",
    "        \"metadata\": {\n",
    "            \"title\": \"Th√©saurus A√©ronautique Final - Wikidata\",\n",
    "            \"description\": \"Th√©saurus exhaustif avec hi√©rarchie et parents imm√©diats\",\n",
    "            \"version\": \"1.0-FINAL\",\n",
    "            \"created\": timestamp,\n",
    "            \"source\": \"Wikidata SPARQL optimis√©\",\n",
    "            \"total_entries\": len(thesaurus_data),\n",
    "            \"extraction_method\": \"multi-query_hierarchical\",\n",
    "            \"parent_detection\": \"automatic_wikidata_relations\",\n",
    "            \"multilingual_support\": True,\n",
    "            \"format\": \"structured_hierarchical_thesaurus\"\n",
    "        },\n",
    "        \"statistics\": stats,\n",
    "        \"data\": thesaurus_data\n",
    "    }\n",
    "    \n",
    "    print(f\"üìÑ Export JSON: {json_filename}\")\n",
    "    with open(json_filepath, 'w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(json_data, jsonfile, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return csv_filepath, json_filepath, stats\n",
    "\n",
    "def analyze_thesaurus_statistics(thesaurus_data):\n",
    "    \"\"\"Analyse les statistiques du th√©saurus final\"\"\"\n",
    "    stats = {\n",
    "        \"total_entries\": len(thesaurus_data),\n",
    "        \"categories\": {},\n",
    "        \"relation_types\": {},\n",
    "        \"languages\": {},\n",
    "        \"hierarchy_depth\": 0,\n",
    "        \"entries_with_synonyms\": 0,\n",
    "        \"entries_with_descriptions\": 0\n",
    "    }\n",
    "    \n",
    "    for entry in thesaurus_data:\n",
    "        # Cat√©gories\n",
    "        category = entry.get(\"category\", \"unknown\")\n",
    "        stats[\"categories\"][category] = stats[\"categories\"].get(category, 0) + 1\n",
    "        \n",
    "        # Types de relation\n",
    "        rel_type = entry.get(\"relation_type\", \"unknown\")\n",
    "        stats[\"relation_types\"][rel_type] = stats[\"relation_types\"].get(rel_type, 0) + 1\n",
    "        \n",
    "        # Langues\n",
    "        lang = entry.get(\"lang\", \"unknown\")\n",
    "        stats[\"languages\"][lang] = stats[\"languages\"].get(lang, 0) + 1\n",
    "        \n",
    "        # Enrichissements\n",
    "        if entry.get(\"synonyms\"):\n",
    "            stats[\"entries_with_synonyms\"] += 1\n",
    "        if entry.get(\"description\"):\n",
    "            stats[\"entries_with_descriptions\"] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def display_final_summary(stats, csv_file, json_file):\n",
    "    \"\"\"Affiche un r√©sum√© final du th√©saurus g√©n√©r√©\"\"\"\n",
    "    print(\"\\nüéØ R√âSUM√â FINAL DU TH√âSAURUS A√âRONAUTIQUE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"üìä STATISTIQUES G√âN√âRALES:\")\n",
    "    print(f\"   ‚Ä¢ Total d'entr√©es: {stats['total_entries']}\")\n",
    "    print(f\"   ‚Ä¢ Entr√©es avec synonymes: {stats['entries_with_synonyms']}\")\n",
    "    print(f\"   ‚Ä¢ Entr√©es avec descriptions: {stats['entries_with_descriptions']}\")\n",
    "    \n",
    "    print(f\"\\nüìÇ R√âPARTITION PAR CAT√âGORIE:\")\n",
    "    for category, count in sorted(stats[\"categories\"].items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / stats['total_entries']) * 100\n",
    "        print(f\"   ‚Ä¢ {category}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüîó TYPES DE RELATIONS:\")\n",
    "    for rel_type, count in sorted(stats[\"relation_types\"].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"   ‚Ä¢ {rel_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\nüåê LANGUES:\")\n",
    "    for lang, count in stats[\"languages\"].items():\n",
    "        print(f\"   ‚Ä¢ {lang}: {count}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ FICHIERS G√âN√âR√âS:\")\n",
    "    print(f\"   ‚úÖ CSV: {os.path.basename(csv_file)}\")\n",
    "    print(f\"   ‚úÖ JSON: {os.path.basename(json_file)}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ MISSION ACCOMPLIE !\")\n",
    "    print(f\" {stats['total_entries']} entr√©es de th√©saurus\")\n",
    "\n",
    "# Export et r√©sum√© final\n",
    "if final_thesaurus:\n",
    "    csv_file, json_file, statistics = export_final_thesaurus(final_thesaurus)\n",
    "    display_final_summary(statistics, csv_file, json_file)\n",
    "else:\n",
    "    print(\"‚ùå Aucun th√©saurus √† exporter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d148f58",
   "metadata": {},
   "source": [
    "### Nettoyage des doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f6dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire le CSV (remplace 'ton_fichier.csv' par le tien)\n",
    "df = pd.read_csv(csv_file, sep=';', dtype=str).fillna('')\n",
    "\n",
    "# Fonction pour concat√©ner les valeurs uniques (s√©par√©es par \"|\")\n",
    "def concat_unique(series):\n",
    "    uniques = set([v.strip() for v in series if v.strip() != ''])\n",
    "    return \" | \".join(sorted(uniques)) if uniques else ''\n",
    "\n",
    "# Grouper par 'ID', en concat√©nant les valeurs diff√©rentes pour chaque colonne\n",
    "df_clean = df.groupby('ID', as_index=False).agg(concat_unique)\n",
    "\n",
    "# Sauvegarder le r√©sultat\n",
    "df_clean.to_csv(csv_file, sep=';', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"‚úÖ CSV nettoy√© et export√© sous {csv_file}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
