{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Scraper Wikidata - Recherche d'entités\n",
    "\n",
    "**Auteur:** May8326  \n",
    "**Date:** 2025-07-02  \n",
    "**Description:** Outil pour scraper toutes les entités Wikidata contenant un terme de recherche spécifique.\n",
    "\n",
    "Ce notebook utilise l'API EntitySearch de Wikidata avec pagination automatique pour récupérer **tous** les résultats sans limitation des 50 premiers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Installation et imports\n",
    "\n",
    "Installation des dépendances nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances (décommentez si nécessaire)\n",
    "# !pip install requests pandas jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Toutes les bibliothèques sont importées avec succès!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from urllib.parse import quote\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Toutes les bibliothèques sont importées avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Configuration\n",
    "\n",
    "Définissez vos paramètres de recherche ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 Langue de recherche: fr\n",
      "📄 Résultats par page: 50\n",
      "⏱️  Délai entre requêtes: 2s\n"
     ]
    }
   ],
   "source": [
    "# Configuration globale\n",
    "LANGUAGE = \"fr\"  # Langue pour la recherche (fr, en, es, etc.)\n",
    "LIMIT = 50       # Nombre de résultats par page (max 50)\n",
    "DELAY = 2        # Délai entre les requêtes (secondes)\n",
    "\n",
    "print(f\"🌍 Langue de recherche: {LANGUAGE}\")\n",
    "print(f\"📄 Résultats par page: {LIMIT}\")\n",
    "print(f\"⏱️  Délai entre requêtes: {DELAY}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Terme de recherche\n",
    "\n",
    "Saisissez le terme que vous souhaitez rechercher dans Wikidata :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Terme de recherche: 'aeronaut'\n"
     ]
    }
   ],
   "source": [
    "# Saisie du terme de recherche\n",
    "search_term = input(\"🔍 Entrez le terme de recherche (par exemple, 'aero'): \")\n",
    "\n",
    "if not search_term.strip():\n",
    "    raise ValueError(\"❌ Le terme de recherche ne peut pas être vide.\")\n",
    "\n",
    "\n",
    "print(f\"✅ Terme de recherche: '{search_term}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Classe WikidataAeroScraper\n",
    "\n",
    "Définition de la classe principale pour le scraping :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classe WikidataAeroScraper définie avec succès!\n"
     ]
    }
   ],
   "source": [
    "class WikidataAeroScraper:\n",
    "    \"\"\"Scraper pour récupérer toutes les entités Wikidata contenant un terme spécifique\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.endpoint = \"https://query.wikidata.org/sparql\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'WikidataBot/1.0 (https://github.com/May8326/wikidata-scraper)',\n",
    "            'Accept': 'application/sparql-results+json'\n",
    "        }\n",
    "        self.results = []\n",
    "        self.stats = {\n",
    "            'total_pages': 0,\n",
    "            'total_results': 0,\n",
    "            'start_time': None,\n",
    "            'end_time': None\n",
    "        }\n",
    "\n",
    "    def build_query(self, search_term, language, limit, offset):\n",
    "        \"\"\"Construit la requête SPARQL avec pagination\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT ?item ?itemLabel WHERE {{\n",
    "          SERVICE wikibase:mwapi {{\n",
    "            bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\n",
    "                            wikibase:api \"EntitySearch\";\n",
    "                            mwapi:search \"{search_term}\";\n",
    "                            mwapi:language \"{language}\";\n",
    "                            mwapi:limit \"{limit}\";\n",
    "                            mwapi:continue \"{offset}\".\n",
    "            ?item wikibase:apiOutputItem mwapi:item.\n",
    "          }}\n",
    "          \n",
    "          SERVICE wikibase:label {{ \n",
    "            bd:serviceParam wikibase:language \"en\". \n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        return query\n",
    "    \n",
    "    def execute_query(self, query):\n",
    "        \"\"\"Exécute une requête SPARQL et retourne les résultats\"\"\"\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                self.endpoint,\n",
    "                params={'query': query, 'format': 'json'},\n",
    "                headers=self.headers,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ Erreur lors de la requête: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_results(self, data):\n",
    "        \"\"\"Extrait les résultats du JSON retourné\"\"\"\n",
    "        if not data or 'results' not in data:\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        for binding in data['results']['bindings']:\n",
    "            item_uri = binding.get('item', {}).get('value', '')\n",
    "            item_label = binding.get('itemLabel', {}).get('value', '')\n",
    "            \n",
    "            # Extraire l'ID Wikidata de l'URI\n",
    "            item_id = item_uri.split('/')[-1] if item_uri else ''\n",
    "            \n",
    "            results.append({\n",
    "                'wikidata_id': item_id,\n",
    "                'label': item_label,\n",
    "                'uri': item_uri\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"✅ Classe WikidataAeroScraper définie avec succès!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pour un scraper plus robuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Méthodes de scraping et sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Méthodes de scraping et sauvegarde ajoutées!\n"
     ]
    }
   ],
   "source": [
    "# Ajout des méthodes de scraping à la classe\n",
    "def scrape_all_results(self):\n",
    "    \"\"\"Scrape tous les résultats en paginant automatiquement\"\"\"\n",
    "    offset = 0\n",
    "    page = 1\n",
    "    total_results = 0\n",
    "    \n",
    "    self.stats['start_time'] = time.time()\n",
    "    \n",
    "    print(f\"🚀 Début du scraping des entités contenant '{search_term}'...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        print(f\"📄 Page {page} (offset: {offset})...\", end=\" \")\n",
    "        \n",
    "        # Construire et exécuter la requête\n",
    "        query = self.build_query(search_term, LANGUAGE, LIMIT, offset)\n",
    "        data = self.execute_query(query)\n",
    "        \n",
    "        if not data:\n",
    "            print(\"❌ Erreur lors de l'exécution de la requête\")\n",
    "            break\n",
    "        \n",
    "        # Extraire les résultats\n",
    "        page_results = self.extract_results(data)\n",
    "        \n",
    "        if not page_results:\n",
    "            print(f\"✅ Fin des résultats atteinte\")\n",
    "            break\n",
    "        \n",
    "        # Ajouter aux résultats totaux\n",
    "        self.results.extend(page_results)\n",
    "        total_results += len(page_results)\n",
    "        \n",
    "        print(f\"[{len(page_results)} résultats] Total: {total_results}\")\n",
    "        \n",
    "        # Vérifier si on a moins de LIMIT résultats (dernière page)\n",
    "        if len(page_results) < LIMIT:\n",
    "            print(f\"✅ Dernière page atteinte ({len(page_results)} < {LIMIT} résultats)\")\n",
    "            break\n",
    "        \n",
    "        # Préparer la page suivante\n",
    "        offset += LIMIT\n",
    "        page += 1\n",
    "        \n",
    "        # Pause pour être respectueux envers l'API\n",
    "        time.sleep(DELAY)\n",
    "    \n",
    "    self.stats['end_time'] = time.time()\n",
    "    self.stats['total_pages'] = page\n",
    "    self.stats['total_results'] = total_results\n",
    "    \n",
    "    duration = self.stats['end_time'] - self.stats['start_time']\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"🎉 Scraping terminé! Total: {total_results} résultats en {duration:.1f}s\")\n",
    "    return self.results\n",
    "\n",
    "def save_to_csv(self, filename=None):\n",
    "    \"\"\"Sauvegarde les résultats en CSV\"\"\"\n",
    "    if not self.results:\n",
    "        print(\"❌ Aucun résultat à sauvegarder\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"wikidata_{search_term}_results.csv\"\n",
    "    \n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['wikidata_id', 'label', 'uri']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for result in self.results:\n",
    "            writer.writerow(result)\n",
    "    \n",
    "    print(f\"💾 Résultats sauvegardés dans: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def save_to_json(self, filename=None):\n",
    "    \"\"\"Sauvegarde les résultats en JSON\"\"\"\n",
    "    if not self.results:\n",
    "        print(\"❌ Aucun résultat à sauvegarder\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"wikidata_{search_term}_results.json\"\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(self.results, jsonfile, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"💾 Résultats sauvegardés dans: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def get_dataframe(self):\n",
    "    \"\"\"Retourne les résultats sous forme de DataFrame pandas\"\"\"\n",
    "    if not self.results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame(self.results)\n",
    "\n",
    "def print_sample_results(self, limit=10):\n",
    "    \"\"\"Affiche un échantillon des résultats\"\"\"\n",
    "    if not self.results:\n",
    "        print(\"❌ Aucun résultat à afficher\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n📋 Échantillon des résultats (premiers {limit}):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(self.results[:limit]):\n",
    "        print(f\"{i+1:2d}. {result['wikidata_id']:12s} | {result['label']}\")\n",
    "    \n",
    "    if len(self.results) > limit:\n",
    "        print(f\"... et {len(self.results) - limit} autres résultats\")\n",
    "\n",
    "def get_stats(self):\n",
    "    \"\"\"Retourne les statistiques du scraping\"\"\"\n",
    "    return self.stats\n",
    "\n",
    "# Ajouter les méthodes à la classe\n",
    "WikidataAeroScraper.scrape_all_results = scrape_all_results\n",
    "WikidataAeroScraper.save_to_csv = save_to_csv\n",
    "WikidataAeroScraper.save_to_json = save_to_json\n",
    "WikidataAeroScraper.get_dataframe = get_dataframe\n",
    "WikidataAeroScraper.print_sample_results = print_sample_results\n",
    "WikidataAeroScraper.get_stats = get_stats\n",
    "\n",
    "print(\"✅ Méthodes de scraping et sauvegarde ajoutées!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Exécution du scraping\n",
    "\n",
    "Lance le processus de scraping complet :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Début du scraping des entités contenant 'aeronaut'...\n",
      "============================================================\n",
      "📄 Page 1 (offset: 0)... ❌ Erreur lors de la requête: 500 Server Error: Internal Server Error for url: https://query.wikidata.org/sparql?query=%0A++++++++SELECT+%3Fitem+%3FitemLabel+WHERE+%7B%0A++++++++++SERVICE+wikibase%3Amwapi+%7B%0A++++++++++++bd%3AserviceParam+wikibase%3Aendpoint+%22www.wikidata.org%22%3B%0A++++++++++++++++++++++++++++wikibase%3Aapi+%22EntitySearch%22%3B%0A++++++++++++++++++++++++++++mwapi%3Asearch+%22aeronaut%22%3B%0A++++++++++++++++++++++++++++mwapi%3Alanguage+%22fr%22%3B%0A++++++++++++++++++++++++++++mwapi%3Alimit+%2250%22%3B%0A++++++++++++++++++++++++++++mwapi%3Acontinue+%220%22.%0A++++++++++++%3Fitem+wikibase%3AapiOutputItem+mwapi%3Aitem.%0A++++++++++%7D%0A%0A++++++++++SERVICE+wikibase%3Alabel+%7B+%0A++++++++++++bd%3AserviceParam+wikibase%3Alanguage+%22en%22.+%0A++++++++++%7D%0A++++++++%7D%0A++++++++&format=json\n",
      "❌ Erreur lors de l'exécution de la requête\n",
      "============================================================\n",
      "🎉 Scraping terminé! Total: 0 résultats en 10.3s\n",
      "❌ Aucun résultat trouvé\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du scraper\n",
    "scraper = WikidataAeroScraper()\n",
    "\n",
    "try:\n",
    "    # Lancer le scraping\n",
    "    results = scraper.scrape_all_results()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n🎯 Mission accomplie! {len(results)} entités récupérées.\")\n",
    "    else:\n",
    "        print(\"❌ Aucun résultat trouvé\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⛔ Arrêt demandé par l'utilisateur\")\n",
    "    if scraper.results:\n",
    "        print(f\"💾 Sauvegarde des {len(scraper.results)} résultats partiels...\")\n",
    "        scraper.save_to_csv(f\"wikidata_{search_term}_partial.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur inattendue: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Débogage du scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DIAGNOSTIC DE L'ARRÊT DU SCRAPING\n",
      "==================================================\n",
      "📊 Résultats obtenus: 22\n",
      "📄 Pages scrapées: 1\n",
      "🔢 Dernier offset utilisé: 0\n",
      "\n",
      "🧪 Test de la page suivante (offset 50)...\n",
      "❌ Erreur lors de la requête: 500 Server Error: Internal Server Error for url: https://query.wikidata.org/sparql?query=%0A++++++++SELECT+%3Fitem+%3FitemLabel+WHERE+%7B%0A++++++++++SERVICE+wikibase%3Amwapi+%7B%0A++++++++++++bd%3AserviceParam+wikibase%3Aendpoint+%22www.wikidata.org%22%3B%0A++++++++++++++++++++++++++++wikibase%3Aapi+%22EntitySearch%22%3B%0A++++++++++++++++++++++++++++mwapi%3Asearch+%22aero%22%3B%0A++++++++++++++++++++++++++++mwapi%3Alanguage+%22fr%22%3B%0A++++++++++++++++++++++++++++mwapi%3Alimit+%2250%22%3B%0A++++++++++++++++++++++++++++mwapi%3Acontinue+%2250%22.%0A++++++++++++%3Fitem+wikibase%3AapiOutputItem+mwapi%3Aitem.%0A++++++++++%7D%0A%0A++++++++++SERVICE+wikibase%3Alabel+%7B+%0A++++++++++++bd%3AserviceParam+wikibase%3Alanguage+%22en%22.+%0A++++++++++%7D%0A++++++++%7D%0A++++++++&format=json\n",
      "   ❌ Erreur lors du test de la page suivante\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Cellule de débogage pour identifier la cause de l'arrêt\n",
    "def debug_scraping_stop(scraper):\n",
    "    \"\"\"Diagnostique pourquoi le scraping s'est arrêté\"\"\"\n",
    "    print(\"🔍 DIAGNOSTIC DE L'ARRÊT DU SCRAPING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Vérifier les statistiques\n",
    "    stats = scraper.get_stats()\n",
    "    total_results = len(scraper.results)\n",
    "    \n",
    "    print(f\"📊 Résultats obtenus: {total_results}\")\n",
    "    print(f\"📄 Pages scrapées: {stats.get('total_pages', 'N/A')}\")\n",
    "    \n",
    "    # Vérifier la dernière page\n",
    "    if total_results > 0:\n",
    "        last_offset = (stats.get('total_pages', 1) - 1) * LIMIT\n",
    "        print(f\"🔢 Dernier offset utilisé: {last_offset}\")\n",
    "        \n",
    "        # Test de la page suivante pour voir s'il y a encore des résultats\n",
    "        print(f\"\\n🧪 Test de la page suivante (offset {last_offset + LIMIT})...\")\n",
    "        \n",
    "        next_query = scraper.build_query(search_term, LANGUAGE, LIMIT, last_offset + LIMIT)\n",
    "        next_data = scraper.execute_query(next_query)\n",
    "        \n",
    "        if next_data:\n",
    "            next_results = scraper.extract_results(next_data)\n",
    "            print(f\"   → {len(next_results)} résultats trouvés sur la page suivante\")\n",
    "            \n",
    "            if len(next_results) == 0:\n",
    "                print(\"   ✅ Confirmation: plus de résultats disponibles\")\n",
    "            else:\n",
    "                print(\"   ⚠️  Il y a encore des résultats ! Le scraping s'est arrêté prématurément\")\n",
    "                \n",
    "                # Afficher quelques exemples de la page suivante\n",
    "                print(\"   📋 Exemples de résultats manqués:\")\n",
    "                for i, result in enumerate(next_results[:5]):\n",
    "                    print(f\"      {i+1}. {result['wikidata_id']} | {result['label']}\")\n",
    "        else:\n",
    "            print(\"   ❌ Erreur lors du test de la page suivante\")\n",
    "    \n",
    "    # Vérifier s'il y a une limite exacte à 10000\n",
    "    if total_results == 10000:\n",
    "        print(f\"\\n⚠️  ALERTE: Arrêt à exactement 10 000 résultats!\")\n",
    "        print(f\"   Cela suggère une limite API côté Wikidata EntitySearch\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Lancer le diagnostic\n",
    "if 'scraper' in locals() and scraper.results:\n",
    "    debug_scraping_stop(scraper)\n",
    "else:\n",
    "    print(\"❌ Aucun scraper avec des résultats trouvé\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essai pour contourner les limites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternative_search_all_results(search_terme, max_retries=3):\n",
    "    \"\"\"\n",
    "    Méthode alternative utilisant plusieurs approches combinées\n",
    "    pour récupérer TOUS les résultats\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Recherche alternative pour '{search_terme}'...\")\n",
    "    \n",
    "    # Méthode 1: EntitySearch classique (jusqu'à 10k)\n",
    "    scraper1 = WikidataAeroScraper()\n",
    "    results1 = scraper1.scrape_all_results()\n",
    "    \n",
    "    print(f\"📊 Méthode 1 (EntitySearch): {len(results1)} résultats\")\n",
    "    \n",
    "    # Méthode 2: Recherche SPARQL directe avec REGEX\n",
    "    query_sparql = f\"\"\"\n",
    "    SELECT DISTINCT ?item ?itemLabel WHERE {{\n",
    "      ?item rdfs:label ?itemLabel .\n",
    "      FILTER(LANG(?itemLabel) = \"en\")\n",
    "      FILTER(REGEX(?itemLabel, \"{search_terme}\", \"i\"))\n",
    "      \n",
    "      SERVICE wikibase:label {{ \n",
    "        bd:serviceParam wikibase:language \"en\". \n",
    "      }}\n",
    "    }}\n",
    "    LIMIT 50000\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🔄 Méthode 2: Recherche SPARQL directe...\")\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'WikidataBot/1.0 (https://github.com/May8326/wikidata-scraper)',\n",
    "        'Accept': 'application/sparql-results+json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            \"https://query.wikidata.org/sparql\",\n",
    "            params={'query': query_sparql, 'format': 'json'},\n",
    "            headers=headers,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            results2 = []\n",
    "            \n",
    "            for binding in data.get('results', {}).get('bindings', []):\n",
    "                item_uri = binding.get('item', {}).get('value', '')\n",
    "                item_label = binding.get('itemLabel', {}).get('value', '')\n",
    "                item_id = item_uri.split('/')[-1] if item_uri else ''\n",
    "                \n",
    "                results2.append({\n",
    "                    'wikidata_id': item_id,\n",
    "                    'label': item_label,\n",
    "                    'uri': item_uri,\n",
    "                    'method': 'sparql_direct'\n",
    "                })\n",
    "            \n",
    "            print(f\"📊 Méthode 2 (SPARQL): {len(results2)} résultats\")\n",
    "        else:\n",
    "            print(f\"❌ Méthode 2 échouée: {response.status_code}\")\n",
    "            results2 = []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur Méthode 2: {e}\")\n",
    "        results2 = []\n",
    "    \n",
    "    # Fusionner et dédupliquer les résultats\n",
    "    all_results = results1.copy()\n",
    "    \n",
    "    # Ajouter les résultats de la méthode 2 qui ne sont pas déjà présents\n",
    "    existing_ids = {r['wikidata_id'] for r in results1}\n",
    "    \n",
    "    new_from_method2 = 0\n",
    "    for result in results2:\n",
    "        if result['wikidata_id'] not in existing_ids:\n",
    "            all_results.append(result)\n",
    "            existing_ids.add(result['wikidata_id'])\n",
    "            new_from_method2 += 1\n",
    "    \n",
    "    print(f\"✅ Total combiné: {len(all_results)} résultats\")\n",
    "    print(f\"   → {len(results1)} de EntitySearch\")\n",
    "    print(f\"   → {new_from_method2} nouveaux de SPARQL direct\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Utilisation\n",
    "# all_results = alternative_search_all_results(search_terme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📱 Affichage avec DataFrame pandas\n",
    "\n",
    "Visualisation moderne avec pandas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 DataFrame avec 22 entrées:\n",
      "Colonnes: ['wikidata_id', 'label', 'uri']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wikidata_id</th>\n",
       "      <th>label</th>\n",
       "      <th>uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q35684</td>\n",
       "      <td>TARDIS</td>\n",
       "      <td>http://www.wikidata.org/entity/Q35684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q565483</td>\n",
       "      <td>3325 TARDIS</td>\n",
       "      <td>http://www.wikidata.org/entity/Q565483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q30897888</td>\n",
       "      <td>traversable acausal retrograde domains in spac...</td>\n",
       "      <td>http://www.wikidata.org/entity/Q30897888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q29043590</td>\n",
       "      <td>Doctor Who: TARDIS</td>\n",
       "      <td>http://www.wikidata.org/entity/Q29043590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q7685721</td>\n",
       "      <td>Tardisode</td>\n",
       "      <td>http://www.wikidata.org/entity/Q7685721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q659822</td>\n",
       "      <td>Mastrils</td>\n",
       "      <td>http://www.wikidata.org/entity/Q659822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q50403238</td>\n",
       "      <td>Traversable acausal retrograde domains in spac...</td>\n",
       "      <td>http://www.wikidata.org/entity/Q50403238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q124372683</td>\n",
       "      <td>TARDIS</td>\n",
       "      <td>http://www.wikidata.org/entity/Q124372683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q106601446</td>\n",
       "      <td>Tardis Data Core</td>\n",
       "      <td>http://www.wikidata.org/entity/Q106601446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q57655398</td>\n",
       "      <td>TARDIS</td>\n",
       "      <td>http://www.wikidata.org/entity/Q57655398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  wikidata_id                                              label  \\\n",
       "0      Q35684                                             TARDIS   \n",
       "1     Q565483                                        3325 TARDIS   \n",
       "2   Q30897888  traversable acausal retrograde domains in spac...   \n",
       "3   Q29043590                                 Doctor Who: TARDIS   \n",
       "4    Q7685721                                          Tardisode   \n",
       "5     Q659822                                           Mastrils   \n",
       "6   Q50403238  Traversable acausal retrograde domains in spac...   \n",
       "7  Q124372683                                             TARDIS   \n",
       "8  Q106601446                                   Tardis Data Core   \n",
       "9   Q57655398                                             TARDIS   \n",
       "\n",
       "                                         uri  \n",
       "0      http://www.wikidata.org/entity/Q35684  \n",
       "1     http://www.wikidata.org/entity/Q565483  \n",
       "2   http://www.wikidata.org/entity/Q30897888  \n",
       "3   http://www.wikidata.org/entity/Q29043590  \n",
       "4    http://www.wikidata.org/entity/Q7685721  \n",
       "5     http://www.wikidata.org/entity/Q659822  \n",
       "6   http://www.wikidata.org/entity/Q50403238  \n",
       "7  http://www.wikidata.org/entity/Q124372683  \n",
       "8  http://www.wikidata.org/entity/Q106601446  \n",
       "9   http://www.wikidata.org/entity/Q57655398  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Informations sur le dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   wikidata_id  22 non-null     object\n",
      " 1   label        22 non-null     object\n",
      " 2   uri          22 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 660.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Conversion en DataFrame pandas\n",
    "if scraper.results:\n",
    "    df = scraper.get_dataframe()\n",
    "    \n",
    "    print(f\"📊 DataFrame avec {len(df)} entrées:\")\n",
    "    print(f\"Colonnes: {list(df.columns)}\")\n",
    "    \n",
    "    # Affichage des premières lignes\n",
    "    display(df.head(10))\n",
    "    \n",
    "    # Informations sur le dataset\n",
    "    print(f\"\\n📋 Informations sur le dataset:\")\n",
    "    print(df.info())\n",
    "else:\n",
    "    print(\"❌ Aucune donnée à afficher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Sauvegarde des résultats\n",
    "\n",
    "Export des données dans différents formats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Résultats sauvegardés dans: wikidata_tardis_results.csv\n",
      "💾 Résultats sauvegardés dans: wikidata_tardis_results.json\n",
      "⚠️  Excel non disponible (installer openpyxl: pip install openpyxl)\n",
      "\n",
      "✅ Tous les fichiers ont été sauvegardés!\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde des résultats\n",
    "if scraper.results:\n",
    "    # Sauvegarde CSV\n",
    "    csv_file = scraper.save_to_csv()\n",
    "    \n",
    "    # Sauvegarde JSON\n",
    "    json_file = scraper.save_to_json()\n",
    "    \n",
    "    # Sauvegarde Excel avec pandas (optionnel)\n",
    "    try:\n",
    "        df = scraper.get_dataframe()\n",
    "        excel_file = f\"wikidata_{search_term}_results.xlsx\"\n",
    "        df.to_excel(excel_file, index=False)\n",
    "        print(f\"💾 Résultats sauvegardés dans: {excel_file}\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️  Excel non disponible (installer openpyxl: pip install openpyxl)\")\n",
    "    \n",
    "    print(f\"\\n✅ Tous les fichiers ont été sauvegardés!\")\n",
    "else:\n",
    "    print(\"❌ Aucun résultat à sauvegarder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Analyse des résultats\n",
    "\n",
    "Quelques analyses simples des données récupérées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyse des résultats:\n",
      "   • Nombre total d'entités: 22\n",
      "   • Nombre d'IDs uniques: 22\n",
      "   • Nombre de labels uniques: 19\n",
      "   • Label le plus long: 51 caractères\n",
      "   • Label le plus court: 6 caractères\n",
      "\n",
      "📏 Top 5 des labels les plus longs:\n",
      "   • Q30897888: traversable acausal retrograde domains in spacetime (51 chars)\n",
      "   • Q50403238: Traversable acausal retrograde domains in spacetime (51 chars)\n",
      "   • Q99441542: TarDiS: Targets and Dynamics in Speech (38 chars)\n",
      "   • Q29043590: Doctor Who: TARDIS (18 chars)\n",
      "   • Q119844953: Tardis in a Field (17 chars)\n",
      "\n",
      "🏷️  Analyse par mots-clés:\n"
     ]
    }
   ],
   "source": [
    "# Analyses des résultats\n",
    "if scraper.results:\n",
    "    df = scraper.get_dataframe()\n",
    "    \n",
    "    print(\"🔍 Analyse des résultats:\")\n",
    "    print(f\"   • Nombre total d'entités: {len(df)}\")\n",
    "    print(f\"   • Nombre d'IDs uniques: {df['wikidata_id'].nunique()}\")\n",
    "    print(f\"   • Nombre de labels uniques: {df['label'].nunique()}\")\n",
    "    \n",
    "    # Labels les plus longs/courts\n",
    "    df['label_length'] = df['label'].str.len()\n",
    "    print(f\"   • Label le plus long: {df['label_length'].max()} caractères\")\n",
    "    print(f\"   • Label le plus court: {df['label_length'].min()} caractères\")\n",
    "    \n",
    "    # Top 10 des labels les plus longs\n",
    "    print(f\"\\n📏 Top 5 des labels les plus longs:\")\n",
    "    longest = df.nlargest(5, 'label_length')[['wikidata_id', 'label', 'label_length']]\n",
    "    for _, row in longest.iterrows():\n",
    "        print(f\"   • {row['wikidata_id']}: {row['label']} ({row['label_length']} chars)\")\n",
    "    \n",
    "    # Recherche de mots-clés spécifiques dans les labels\n",
    "    keywords = ['company', 'airport', 'aircraft', 'airline']\n",
    "    print(f\"\\n🏷️  Analyse par mots-clés:\")\n",
    "    for keyword in keywords:\n",
    "        count = df['label'].str.contains(keyword, case=False, na=False).sum()\n",
    "        if count > 0:\n",
    "            print(f\"   • '{keyword}': {count} entités\")\n",
    "else:\n",
    "    print(\"❌ Aucune donnée à analyser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Résumé final\n",
    "\n",
    "Bilan de l'exécution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 RÉSUMÉ FINAL\n",
      "==================================================\n",
      "Terme recherché: 'tardis'\n",
      "Langue: fr\n",
      "Résultats trouvés: 22\n",
      "Durée d'exécution: 0.4 secondes\n",
      "Pages scrapées: 1\n",
      "\n",
      "📁 Fichiers générés:\n",
      "   • wikidata_tardis_results.csv\n",
      "   • wikidata_tardis_results.json\n",
      "   • wikidata_tardis_results.xlsx (si disponible)\n",
      "\n",
      "✅ Scraping terminé avec succès!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Résumé final\n",
    "print(\"🎯 RÉSUMÉ FINAL\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Terme recherché: '{search_term}'\")\n",
    "print(f\"Langue: {LANGUAGE}\")\n",
    "print(f\"Résultats trouvés: {len(scraper.results) if scraper.results else 0}\")\n",
    "\n",
    "if scraper.results:\n",
    "    stats = scraper.get_stats()\n",
    "    if stats['start_time'] and stats['end_time']:\n",
    "        duration = stats['end_time'] - stats['start_time']\n",
    "        print(f\"Durée d'exécution: {duration:.1f} secondes\")\n",
    "        print(f\"Pages scrapées: {stats['total_pages']}\")\n",
    "    \n",
    "    print(f\"\\n📁 Fichiers générés:\")\n",
    "    print(f\"   • wikidata_{search_term}_results.csv\")\n",
    "    print(f\"   • wikidata_{search_term}_results.json\")\n",
    "    print(f\"   • wikidata_{search_term}_results.xlsx (si disponible)\")\n",
    "    \n",
    "    print(f\"\\n✅ Scraping terminé avec succès!\")\n",
    "else:\n",
    "    print(f\"\\n❌ Aucun résultat trouvé pour '{search_term}'\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
