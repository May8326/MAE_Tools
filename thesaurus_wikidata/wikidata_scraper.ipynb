{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Scraper Wikidata - Recherche d'entit√©s\n",
    "\n",
    "**Auteur:** May8326  \n",
    "**Date:** 2025-07-02  \n",
    "**Description:** Outil pour scraper toutes les entit√©s Wikidata contenant un terme de recherche sp√©cifique.\n",
    "\n",
    "Ce notebook utilise l'API EntitySearch de Wikidata avec pagination automatique pour r√©cup√©rer **tous** les r√©sultats sans limitation des 50 premiers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation et imports\n",
    "\n",
    "Installation des d√©pendances n√©cessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (d√©commentez si n√©cessaire)\n",
    "# !pip install requests pandas jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Toutes les biblioth√®ques sont import√©es avec succ√®s!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from urllib.parse import quote\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Toutes les biblioth√®ques sont import√©es avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "D√©finissez vos param√®tres de recherche ici :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç Langue de recherche: fr\n",
      "üìÑ R√©sultats par page: 50\n",
      "‚è±Ô∏è  D√©lai entre requ√™tes: 2s\n"
     ]
    }
   ],
   "source": [
    "# Configuration globale\n",
    "LANGUAGE = \"fr\"  # Langue pour la recherche (fr, en, es, etc.)\n",
    "LIMIT = 50       # Nombre de r√©sultats par page (max 50)\n",
    "DELAY = 2        # D√©lai entre les requ√™tes (secondes)\n",
    "\n",
    "print(f\"üåç Langue de recherche: {LANGUAGE}\")\n",
    "print(f\"üìÑ R√©sultats par page: {LIMIT}\")\n",
    "print(f\"‚è±Ô∏è  D√©lai entre requ√™tes: {DELAY}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Terme de recherche\n",
    "\n",
    "Saisissez le terme que vous souhaitez rechercher dans Wikidata :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Terme de recherche: 'aeronaut'\n"
     ]
    }
   ],
   "source": [
    "# Saisie du terme de recherche\n",
    "search_term = input(\"üîç Entrez le terme de recherche (par exemple, 'aero'): \")\n",
    "\n",
    "if not search_term.strip():\n",
    "    raise ValueError(\"‚ùå Le terme de recherche ne peut pas √™tre vide.\")\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Terme de recherche: '{search_term}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Classe WikidataAeroScraper\n",
    "\n",
    "D√©finition de la classe principale pour le scraping :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classe WikidataAeroScraper d√©finie avec succ√®s!\n"
     ]
    }
   ],
   "source": [
    "class WikidataAeroScraper:\n",
    "    \"\"\"Scraper pour r√©cup√©rer toutes les entit√©s Wikidata contenant un terme sp√©cifique\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.endpoint = \"https://query.wikidata.org/sparql\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'WikidataBot/1.0 (https://github.com/May8326/wikidata-scraper)',\n",
    "            'Accept': 'application/sparql-results+json'\n",
    "        }\n",
    "        self.results = []\n",
    "        self.stats = {\n",
    "            'total_pages': 0,\n",
    "            'total_results': 0,\n",
    "            'start_time': None,\n",
    "            'end_time': None\n",
    "        }\n",
    "\n",
    "    def build_query(self, search_term, language, limit, offset):\n",
    "        \"\"\"Construit la requ√™te SPARQL avec pagination\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT ?item ?itemLabel WHERE {{\n",
    "          SERVICE wikibase:mwapi {{\n",
    "            bd:serviceParam wikibase:endpoint \"www.wikidata.org\";\n",
    "                            wikibase:api \"EntitySearch\";\n",
    "                            mwapi:search \"{search_term}\";\n",
    "                            mwapi:language \"{language}\";\n",
    "                            mwapi:limit \"{limit}\";\n",
    "                            mwapi:continue \"{offset}\".\n",
    "            ?item wikibase:apiOutputItem mwapi:item.\n",
    "          }}\n",
    "          \n",
    "          SERVICE wikibase:label {{ \n",
    "            bd:serviceParam wikibase:language \"en\". \n",
    "          }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        return query\n",
    "    \n",
    "    def execute_query(self, query):\n",
    "        \"\"\"Ex√©cute une requ√™te SPARQL et retourne les r√©sultats\"\"\"\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                self.endpoint,\n",
    "                params={'query': query, 'format': 'json'},\n",
    "                headers=self.headers,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Erreur lors de la requ√™te: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_results(self, data):\n",
    "        \"\"\"Extrait les r√©sultats du JSON retourn√©\"\"\"\n",
    "        if not data or 'results' not in data:\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        for binding in data['results']['bindings']:\n",
    "            item_uri = binding.get('item', {}).get('value', '')\n",
    "            item_label = binding.get('itemLabel', {}).get('value', '')\n",
    "            \n",
    "            # Extraire l'ID Wikidata de l'URI\n",
    "            item_id = item_uri.split('/')[-1] if item_uri else ''\n",
    "            \n",
    "            results.append({\n",
    "                'wikidata_id': item_id,\n",
    "                'label': item_label,\n",
    "                'uri': item_uri\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úÖ Classe WikidataAeroScraper d√©finie avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test pour un scraper plus robuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ M√©thodes de scraping et sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ M√©thodes de scraping et sauvegarde ajout√©es!\n"
     ]
    }
   ],
   "source": [
    "# Ajout des m√©thodes de scraping √† la classe\n",
    "def scrape_all_results(self):\n",
    "    \"\"\"Scrape tous les r√©sultats en paginant automatiquement\"\"\"\n",
    "    offset = 0\n",
    "    page = 1\n",
    "    total_results = 0\n",
    "    \n",
    "    self.stats['start_time'] = time.time()\n",
    "    \n",
    "    print(f\"üöÄ D√©but du scraping des entit√©s contenant '{search_term}'...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        print(f\"üìÑ Page {page} (offset: {offset})...\", end=\" \")\n",
    "        \n",
    "        # Construire et ex√©cuter la requ√™te\n",
    "        query = self.build_query(search_term, LANGUAGE, LIMIT, offset)\n",
    "        data = self.execute_query(query)\n",
    "        \n",
    "        if not data:\n",
    "            print(\"‚ùå Erreur lors de l'ex√©cution de la requ√™te\")\n",
    "            break\n",
    "        \n",
    "        # Extraire les r√©sultats\n",
    "        page_results = self.extract_results(data)\n",
    "        \n",
    "        if not page_results:\n",
    "            print(f\"‚úÖ Fin des r√©sultats atteinte\")\n",
    "            break\n",
    "        \n",
    "        # Ajouter aux r√©sultats totaux\n",
    "        self.results.extend(page_results)\n",
    "        total_results += len(page_results)\n",
    "        \n",
    "        print(f\"[{len(page_results)} r√©sultats] Total: {total_results}\")\n",
    "        \n",
    "        # V√©rifier si on a moins de LIMIT r√©sultats (derni√®re page)\n",
    "        if len(page_results) < LIMIT:\n",
    "            print(f\"‚úÖ Derni√®re page atteinte ({len(page_results)} < {LIMIT} r√©sultats)\")\n",
    "            break\n",
    "        \n",
    "        # Pr√©parer la page suivante\n",
    "        offset += LIMIT\n",
    "        page += 1\n",
    "        \n",
    "        # Pause pour √™tre respectueux envers l'API\n",
    "        time.sleep(DELAY)\n",
    "    \n",
    "    self.stats['end_time'] = time.time()\n",
    "    self.stats['total_pages'] = page\n",
    "    self.stats['total_results'] = total_results\n",
    "    \n",
    "    duration = self.stats['end_time'] - self.stats['start_time']\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üéâ Scraping termin√©! Total: {total_results} r√©sultats en {duration:.1f}s\")\n",
    "    return self.results\n",
    "\n",
    "def save_to_csv(self, filename=None):\n",
    "    \"\"\"Sauvegarde les r√©sultats en CSV\"\"\"\n",
    "    if not self.results:\n",
    "        print(\"‚ùå Aucun r√©sultat √† sauvegarder\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"wikidata_{search_term}_results.csv\"\n",
    "    \n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['wikidata_id', 'label', 'uri']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for result in self.results:\n",
    "            writer.writerow(result)\n",
    "    \n",
    "    print(f\"üíæ R√©sultats sauvegard√©s dans: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def save_to_json(self, filename=None):\n",
    "    \"\"\"Sauvegarde les r√©sultats en JSON\"\"\"\n",
    "    if not self.results:\n",
    "        print(\"‚ùå Aucun r√©sultat √† sauvegarder\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"wikidata_{search_term}_results.json\"\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(self.results, jsonfile, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"üíæ R√©sultats sauvegard√©s dans: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def get_dataframe(self):\n",
    "    \"\"\"Retourne les r√©sultats sous forme de DataFrame pandas\"\"\"\n",
    "    if not self.results:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame(self.results)\n",
    "\n",
    "def print_sample_results(self, limit=10):\n",
    "    \"\"\"Affiche un √©chantillon des r√©sultats\"\"\"\n",
    "    if not self.results:\n",
    "        print(\"‚ùå Aucun r√©sultat √† afficher\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìã √âchantillon des r√©sultats (premiers {limit}):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(self.results[:limit]):\n",
    "        print(f\"{i+1:2d}. {result['wikidata_id']:12s} | {result['label']}\")\n",
    "    \n",
    "    if len(self.results) > limit:\n",
    "        print(f\"... et {len(self.results) - limit} autres r√©sultats\")\n",
    "\n",
    "def get_stats(self):\n",
    "    \"\"\"Retourne les statistiques du scraping\"\"\"\n",
    "    return self.stats\n",
    "\n",
    "# Ajouter les m√©thodes √† la classe\n",
    "WikidataAeroScraper.scrape_all_results = scrape_all_results\n",
    "WikidataAeroScraper.save_to_csv = save_to_csv\n",
    "WikidataAeroScraper.save_to_json = save_to_json\n",
    "WikidataAeroScraper.get_dataframe = get_dataframe\n",
    "WikidataAeroScraper.print_sample_results = print_sample_results\n",
    "WikidataAeroScraper.get_stats = get_stats\n",
    "\n",
    "print(\"‚úÖ M√©thodes de scraping et sauvegarde ajout√©es!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Ex√©cution du scraping\n",
    "\n",
    "Lance le processus de scraping complet :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©but du scraping des entit√©s contenant 'aeronaut'...\n",
      "============================================================\n",
      "üìÑ Page 1 (offset: 0)... ‚ùå Erreur lors de la requ√™te: 500 Server Error: Internal Server Error for url: https://query.wikidata.org/sparql?query=%0A++++++++SELECT+%3Fitem+%3FitemLabel+WHERE+%7B%0A++++++++++SERVICE+wikibase%3Amwapi+%7B%0A++++++++++++bd%3AserviceParam+wikibase%3Aendpoint+%22www.wikidata.org%22%3B%0A++++++++++++++++++++++++++++wikibase%3Aapi+%22EntitySearch%22%3B%0A++++++++++++++++++++++++++++mwapi%3Asearch+%22aeronaut%22%3B%0A++++++++++++++++++++++++++++mwapi%3Alanguage+%22fr%22%3B%0A++++++++++++++++++++++++++++mwapi%3Alimit+%2250%22%3B%0A++++++++++++++++++++++++++++mwapi%3Acontinue+%220%22.%0A++++++++++++%3Fitem+wikibase%3AapiOutputItem+mwapi%3Aitem.%0A++++++++++%7D%0A%0A++++++++++SERVICE+wikibase%3Alabel+%7B+%0A++++++++++++bd%3AserviceParam+wikibase%3Alanguage+%22en%22.+%0A++++++++++%7D%0A++++++++%7D%0A++++++++&format=json\n",
      "‚ùå Erreur lors de l'ex√©cution de la requ√™te\n",
      "============================================================\n",
      "üéâ Scraping termin√©! Total: 0 r√©sultats en 10.3s\n",
      "‚ùå Aucun r√©sultat trouv√©\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du scraper\n",
    "scraper = WikidataAeroScraper()\n",
    "\n",
    "try:\n",
    "    # Lancer le scraping\n",
    "    results = scraper.scrape_all_results()\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nüéØ Mission accomplie! {len(results)} entit√©s r√©cup√©r√©es.\")\n",
    "    else:\n",
    "        print(\"‚ùå Aucun r√©sultat trouv√©\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚õî Arr√™t demand√© par l'utilisateur\")\n",
    "    if scraper.results:\n",
    "        print(f\"üíæ Sauvegarde des {len(scraper.results)} r√©sultats partiels...\")\n",
    "        scraper.save_to_csv(f\"wikidata_{search_term}_partial.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur inattendue: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D√©bogage du scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DIAGNOSTIC DE L'ARR√äT DU SCRAPING\n",
      "==================================================\n",
      "üìä R√©sultats obtenus: 22\n",
      "üìÑ Pages scrap√©es: 1\n",
      "üî¢ Dernier offset utilis√©: 0\n",
      "\n",
      "üß™ Test de la page suivante (offset 50)...\n",
      "‚ùå Erreur lors de la requ√™te: 500 Server Error: Internal Server Error for url: https://query.wikidata.org/sparql?query=%0A++++++++SELECT+%3Fitem+%3FitemLabel+WHERE+%7B%0A++++++++++SERVICE+wikibase%3Amwapi+%7B%0A++++++++++++bd%3AserviceParam+wikibase%3Aendpoint+%22www.wikidata.org%22%3B%0A++++++++++++++++++++++++++++wikibase%3Aapi+%22EntitySearch%22%3B%0A++++++++++++++++++++++++++++mwapi%3Asearch+%22aero%22%3B%0A++++++++++++++++++++++++++++mwapi%3Alanguage+%22fr%22%3B%0A++++++++++++++++++++++++++++mwapi%3Alimit+%2250%22%3B%0A++++++++++++++++++++++++++++mwapi%3Acontinue+%2250%22.%0A++++++++++++%3Fitem+wikibase%3AapiOutputItem+mwapi%3Aitem.%0A++++++++++%7D%0A%0A++++++++++SERVICE+wikibase%3Alabel+%7B+%0A++++++++++++bd%3AserviceParam+wikibase%3Alanguage+%22en%22.+%0A++++++++++%7D%0A++++++++%7D%0A++++++++&format=json\n",
      "   ‚ùå Erreur lors du test de la page suivante\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Cellule de d√©bogage pour identifier la cause de l'arr√™t\n",
    "def debug_scraping_stop(scraper):\n",
    "    \"\"\"Diagnostique pourquoi le scraping s'est arr√™t√©\"\"\"\n",
    "    print(\"üîç DIAGNOSTIC DE L'ARR√äT DU SCRAPING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # V√©rifier les statistiques\n",
    "    stats = scraper.get_stats()\n",
    "    total_results = len(scraper.results)\n",
    "    \n",
    "    print(f\"üìä R√©sultats obtenus: {total_results}\")\n",
    "    print(f\"üìÑ Pages scrap√©es: {stats.get('total_pages', 'N/A')}\")\n",
    "    \n",
    "    # V√©rifier la derni√®re page\n",
    "    if total_results > 0:\n",
    "        last_offset = (stats.get('total_pages', 1) - 1) * LIMIT\n",
    "        print(f\"üî¢ Dernier offset utilis√©: {last_offset}\")\n",
    "        \n",
    "        # Test de la page suivante pour voir s'il y a encore des r√©sultats\n",
    "        print(f\"\\nüß™ Test de la page suivante (offset {last_offset + LIMIT})...\")\n",
    "        \n",
    "        next_query = scraper.build_query(search_term, LANGUAGE, LIMIT, last_offset + LIMIT)\n",
    "        next_data = scraper.execute_query(next_query)\n",
    "        \n",
    "        if next_data:\n",
    "            next_results = scraper.extract_results(next_data)\n",
    "            print(f\"   ‚Üí {len(next_results)} r√©sultats trouv√©s sur la page suivante\")\n",
    "            \n",
    "            if len(next_results) == 0:\n",
    "                print(\"   ‚úÖ Confirmation: plus de r√©sultats disponibles\")\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è  Il y a encore des r√©sultats ! Le scraping s'est arr√™t√© pr√©matur√©ment\")\n",
    "                \n",
    "                # Afficher quelques exemples de la page suivante\n",
    "                print(\"   üìã Exemples de r√©sultats manqu√©s:\")\n",
    "                for i, result in enumerate(next_results[:5]):\n",
    "                    print(f\"      {i+1}. {result['wikidata_id']} | {result['label']}\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Erreur lors du test de la page suivante\")\n",
    "    \n",
    "    # V√©rifier s'il y a une limite exacte √† 10000\n",
    "    if total_results == 10000:\n",
    "        print(f\"\\n‚ö†Ô∏è  ALERTE: Arr√™t √† exactement 10 000 r√©sultats!\")\n",
    "        print(f\"   Cela sugg√®re une limite API c√¥t√© Wikidata EntitySearch\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Lancer le diagnostic\n",
    "if 'scraper' in locals() and scraper.results:\n",
    "    debug_scraping_stop(scraper)\n",
    "else:\n",
    "    print(\"‚ùå Aucun scraper avec des r√©sultats trouv√©\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essai pour contourner les limites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternative_search_all_results(search_terme, max_retries=3):\n",
    "    \"\"\"\n",
    "    M√©thode alternative utilisant plusieurs approches combin√©es\n",
    "    pour r√©cup√©rer TOUS les r√©sultats\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Recherche alternative pour '{search_terme}'...\")\n",
    "    \n",
    "    # M√©thode 1: EntitySearch classique (jusqu'√† 10k)\n",
    "    scraper1 = WikidataAeroScraper()\n",
    "    results1 = scraper1.scrape_all_results()\n",
    "    \n",
    "    print(f\"üìä M√©thode 1 (EntitySearch): {len(results1)} r√©sultats\")\n",
    "    \n",
    "    # M√©thode 2: Recherche SPARQL directe avec REGEX\n",
    "    query_sparql = f\"\"\"\n",
    "    SELECT DISTINCT ?item ?itemLabel WHERE {{\n",
    "      ?item rdfs:label ?itemLabel .\n",
    "      FILTER(LANG(?itemLabel) = \"en\")\n",
    "      FILTER(REGEX(?itemLabel, \"{search_terme}\", \"i\"))\n",
    "      \n",
    "      SERVICE wikibase:label {{ \n",
    "        bd:serviceParam wikibase:language \"en\". \n",
    "      }}\n",
    "    }}\n",
    "    LIMIT 50000\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîÑ M√©thode 2: Recherche SPARQL directe...\")\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'WikidataBot/1.0 (https://github.com/May8326/wikidata-scraper)',\n",
    "        'Accept': 'application/sparql-results+json'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            \"https://query.wikidata.org/sparql\",\n",
    "            params={'query': query_sparql, 'format': 'json'},\n",
    "            headers=headers,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            results2 = []\n",
    "            \n",
    "            for binding in data.get('results', {}).get('bindings', []):\n",
    "                item_uri = binding.get('item', {}).get('value', '')\n",
    "                item_label = binding.get('itemLabel', {}).get('value', '')\n",
    "                item_id = item_uri.split('/')[-1] if item_uri else ''\n",
    "                \n",
    "                results2.append({\n",
    "                    'wikidata_id': item_id,\n",
    "                    'label': item_label,\n",
    "                    'uri': item_uri,\n",
    "                    'method': 'sparql_direct'\n",
    "                })\n",
    "            \n",
    "            print(f\"üìä M√©thode 2 (SPARQL): {len(results2)} r√©sultats\")\n",
    "        else:\n",
    "            print(f\"‚ùå M√©thode 2 √©chou√©e: {response.status_code}\")\n",
    "            results2 = []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur M√©thode 2: {e}\")\n",
    "        results2 = []\n",
    "    \n",
    "    # Fusionner et d√©dupliquer les r√©sultats\n",
    "    all_results = results1.copy()\n",
    "    \n",
    "    # Ajouter les r√©sultats de la m√©thode 2 qui ne sont pas d√©j√† pr√©sents\n",
    "    existing_ids = {r['wikidata_id'] for r in results1}\n",
    "    \n",
    "    new_from_method2 = 0\n",
    "    for result in results2:\n",
    "        if result['wikidata_id'] not in existing_ids:\n",
    "            all_results.append(result)\n",
    "            existing_ids.add(result['wikidata_id'])\n",
    "            new_from_method2 += 1\n",
    "    \n",
    "    print(f\"‚úÖ Total combin√©: {len(all_results)} r√©sultats\")\n",
    "    print(f\"   ‚Üí {len(results1)} de EntitySearch\")\n",
    "    print(f\"   ‚Üí {new_from_method2} nouveaux de SPARQL direct\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Utilisation\n",
    "# all_results = alternative_search_all_results(search_terme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì± Affichage avec DataFrame pandas\n",
    "\n",
    "Visualisation moderne avec pandas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DataFrame avec 22 entr√©es:\n",
      "Colonnes: ['wikidata_id', 'label', 'uri']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wikidata_id</th>\n",
       "      <th>label</th>\n",
       "      <th>uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q35684</td>\n",
       "      <td>TARDIS</td>\n",
       "      <td>http://www.wikidata.org/entity/Q35684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q565483</td>\n",
       "      <td>3325 TARDIS</td>\n",
       "      <td>http://www.wikidata.org/entity/Q565483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q30897888</td>\n",
       "      <td>traversable acausal retrograde domains in spac...</td>\n",
       "      <td>http://www.wikidata.org/entity/Q30897888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q29043590</td>\n",
       "      <td>Doctor Who: TARDIS</td>\n",
       "      <td>http://www.wikidata.org/entity/Q29043590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q7685721</td>\n",
       "      <td>Tardisode</td>\n",
       "      <td>http://www.wikidata.org/entity/Q7685721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Q659822</td>\n",
       "      <td>Mastrils</td>\n",
       "      <td>http://www.wikidata.org/entity/Q659822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Q50403238</td>\n",
       "      <td>Traversable acausal retrograde domains in spac...</td>\n",
       "      <td>http://www.wikidata.org/entity/Q50403238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Q124372683</td>\n",
       "      <td>TARDIS</td>\n",
       "      <td>http://www.wikidata.org/entity/Q124372683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q106601446</td>\n",
       "      <td>Tardis Data Core</td>\n",
       "      <td>http://www.wikidata.org/entity/Q106601446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q57655398</td>\n",
       "      <td>TARDIS</td>\n",
       "      <td>http://www.wikidata.org/entity/Q57655398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  wikidata_id                                              label  \\\n",
       "0      Q35684                                             TARDIS   \n",
       "1     Q565483                                        3325 TARDIS   \n",
       "2   Q30897888  traversable acausal retrograde domains in spac...   \n",
       "3   Q29043590                                 Doctor Who: TARDIS   \n",
       "4    Q7685721                                          Tardisode   \n",
       "5     Q659822                                           Mastrils   \n",
       "6   Q50403238  Traversable acausal retrograde domains in spac...   \n",
       "7  Q124372683                                             TARDIS   \n",
       "8  Q106601446                                   Tardis Data Core   \n",
       "9   Q57655398                                             TARDIS   \n",
       "\n",
       "                                         uri  \n",
       "0      http://www.wikidata.org/entity/Q35684  \n",
       "1     http://www.wikidata.org/entity/Q565483  \n",
       "2   http://www.wikidata.org/entity/Q30897888  \n",
       "3   http://www.wikidata.org/entity/Q29043590  \n",
       "4    http://www.wikidata.org/entity/Q7685721  \n",
       "5     http://www.wikidata.org/entity/Q659822  \n",
       "6   http://www.wikidata.org/entity/Q50403238  \n",
       "7  http://www.wikidata.org/entity/Q124372683  \n",
       "8  http://www.wikidata.org/entity/Q106601446  \n",
       "9   http://www.wikidata.org/entity/Q57655398  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Informations sur le dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22 entries, 0 to 21\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   wikidata_id  22 non-null     object\n",
      " 1   label        22 non-null     object\n",
      " 2   uri          22 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 660.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Conversion en DataFrame pandas\n",
    "if scraper.results:\n",
    "    df = scraper.get_dataframe()\n",
    "    \n",
    "    print(f\"üìä DataFrame avec {len(df)} entr√©es:\")\n",
    "    print(f\"Colonnes: {list(df.columns)}\")\n",
    "    \n",
    "    # Affichage des premi√®res lignes\n",
    "    display(df.head(10))\n",
    "    \n",
    "    # Informations sur le dataset\n",
    "    print(f\"\\nüìã Informations sur le dataset:\")\n",
    "    print(df.info())\n",
    "else:\n",
    "    print(\"‚ùå Aucune donn√©e √† afficher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Sauvegarde des r√©sultats\n",
    "\n",
    "Export des donn√©es dans diff√©rents formats :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ R√©sultats sauvegard√©s dans: wikidata_tardis_results.csv\n",
      "üíæ R√©sultats sauvegard√©s dans: wikidata_tardis_results.json\n",
      "‚ö†Ô∏è  Excel non disponible (installer openpyxl: pip install openpyxl)\n",
      "\n",
      "‚úÖ Tous les fichiers ont √©t√© sauvegard√©s!\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde des r√©sultats\n",
    "if scraper.results:\n",
    "    # Sauvegarde CSV\n",
    "    csv_file = scraper.save_to_csv()\n",
    "    \n",
    "    # Sauvegarde JSON\n",
    "    json_file = scraper.save_to_json()\n",
    "    \n",
    "    # Sauvegarde Excel avec pandas (optionnel)\n",
    "    try:\n",
    "        df = scraper.get_dataframe()\n",
    "        excel_file = f\"wikidata_{search_term}_results.xlsx\"\n",
    "        df.to_excel(excel_file, index=False)\n",
    "        print(f\"üíæ R√©sultats sauvegard√©s dans: {excel_file}\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Excel non disponible (installer openpyxl: pip install openpyxl)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Tous les fichiers ont √©t√© sauvegard√©s!\")\n",
    "else:\n",
    "    print(\"‚ùå Aucun r√©sultat √† sauvegarder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Analyse des r√©sultats\n",
    "\n",
    "Quelques analyses simples des donn√©es r√©cup√©r√©es :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyse des r√©sultats:\n",
      "   ‚Ä¢ Nombre total d'entit√©s: 22\n",
      "   ‚Ä¢ Nombre d'IDs uniques: 22\n",
      "   ‚Ä¢ Nombre de labels uniques: 19\n",
      "   ‚Ä¢ Label le plus long: 51 caract√®res\n",
      "   ‚Ä¢ Label le plus court: 6 caract√®res\n",
      "\n",
      "üìè Top 5 des labels les plus longs:\n",
      "   ‚Ä¢ Q30897888: traversable acausal retrograde domains in spacetime (51 chars)\n",
      "   ‚Ä¢ Q50403238: Traversable acausal retrograde domains in spacetime (51 chars)\n",
      "   ‚Ä¢ Q99441542: TarDiS: Targets and Dynamics in Speech (38 chars)\n",
      "   ‚Ä¢ Q29043590: Doctor Who: TARDIS (18 chars)\n",
      "   ‚Ä¢ Q119844953: Tardis in a Field (17 chars)\n",
      "\n",
      "üè∑Ô∏è  Analyse par mots-cl√©s:\n"
     ]
    }
   ],
   "source": [
    "# Analyses des r√©sultats\n",
    "if scraper.results:\n",
    "    df = scraper.get_dataframe()\n",
    "    \n",
    "    print(\"üîç Analyse des r√©sultats:\")\n",
    "    print(f\"   ‚Ä¢ Nombre total d'entit√©s: {len(df)}\")\n",
    "    print(f\"   ‚Ä¢ Nombre d'IDs uniques: {df['wikidata_id'].nunique()}\")\n",
    "    print(f\"   ‚Ä¢ Nombre de labels uniques: {df['label'].nunique()}\")\n",
    "    \n",
    "    # Labels les plus longs/courts\n",
    "    df['label_length'] = df['label'].str.len()\n",
    "    print(f\"   ‚Ä¢ Label le plus long: {df['label_length'].max()} caract√®res\")\n",
    "    print(f\"   ‚Ä¢ Label le plus court: {df['label_length'].min()} caract√®res\")\n",
    "    \n",
    "    # Top 10 des labels les plus longs\n",
    "    print(f\"\\nüìè Top 5 des labels les plus longs:\")\n",
    "    longest = df.nlargest(5, 'label_length')[['wikidata_id', 'label', 'label_length']]\n",
    "    for _, row in longest.iterrows():\n",
    "        print(f\"   ‚Ä¢ {row['wikidata_id']}: {row['label']} ({row['label_length']} chars)\")\n",
    "    \n",
    "    # Recherche de mots-cl√©s sp√©cifiques dans les labels\n",
    "    keywords = ['company', 'airport', 'aircraft', 'airline']\n",
    "    print(f\"\\nüè∑Ô∏è  Analyse par mots-cl√©s:\")\n",
    "    for keyword in keywords:\n",
    "        count = df['label'].str.contains(keyword, case=False, na=False).sum()\n",
    "        if count > 0:\n",
    "            print(f\"   ‚Ä¢ '{keyword}': {count} entit√©s\")\n",
    "else:\n",
    "    print(\"‚ùå Aucune donn√©e √† analyser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ R√©sum√© final\n",
    "\n",
    "Bilan de l'ex√©cution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ R√âSUM√â FINAL\n",
      "==================================================\n",
      "Terme recherch√©: 'tardis'\n",
      "Langue: fr\n",
      "R√©sultats trouv√©s: 22\n",
      "Dur√©e d'ex√©cution: 0.4 secondes\n",
      "Pages scrap√©es: 1\n",
      "\n",
      "üìÅ Fichiers g√©n√©r√©s:\n",
      "   ‚Ä¢ wikidata_tardis_results.csv\n",
      "   ‚Ä¢ wikidata_tardis_results.json\n",
      "   ‚Ä¢ wikidata_tardis_results.xlsx (si disponible)\n",
      "\n",
      "‚úÖ Scraping termin√© avec succ√®s!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# R√©sum√© final\n",
    "print(\"üéØ R√âSUM√â FINAL\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Terme recherch√©: '{search_term}'\")\n",
    "print(f\"Langue: {LANGUAGE}\")\n",
    "print(f\"R√©sultats trouv√©s: {len(scraper.results) if scraper.results else 0}\")\n",
    "\n",
    "if scraper.results:\n",
    "    stats = scraper.get_stats()\n",
    "    if stats['start_time'] and stats['end_time']:\n",
    "        duration = stats['end_time'] - stats['start_time']\n",
    "        print(f\"Dur√©e d'ex√©cution: {duration:.1f} secondes\")\n",
    "        print(f\"Pages scrap√©es: {stats['total_pages']}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Fichiers g√©n√©r√©s:\")\n",
    "    print(f\"   ‚Ä¢ wikidata_{search_term}_results.csv\")\n",
    "    print(f\"   ‚Ä¢ wikidata_{search_term}_results.json\")\n",
    "    print(f\"   ‚Ä¢ wikidata_{search_term}_results.xlsx (si disponible)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Scraping termin√© avec succ√®s!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Aucun r√©sultat trouv√© pour '{search_term}'\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
